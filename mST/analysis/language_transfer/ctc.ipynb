{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from fairseq.data import Dictionary\n",
    "from fairseq.models.mST.w2v2_phone_transformer import W2V2Transformer\n",
    "from fairseq.data.audio.multilingual_triplet_v2_phone_dataset import (\n",
    "    MultilingualTripletDataConfig,\n",
    "    MultilingualTripletDataset,\n",
    "    MultilingualTripletDatasetCreator\n",
    ")\n",
    "from fairseq.data.audio.speech_to_text_dataset import get_features_or_waveform\n",
    "from examples.speech_to_text.data_utils import load_df_from_tsv\n",
    "from fairseq.checkpoint_utils import load_checkpoint_to_cpu\n",
    "from fairseq.data.encoders.sentencepiece_bpe import SentencepieceBPE, SentencepieceConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font=\"Noto Sans CJK JP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "task = Namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(vocab_filename):\n",
    "    _dict_path = vocab_filename\n",
    "    if not os.path.isfile(_dict_path):\n",
    "        raise FileNotFoundError(f\"Dict not found: {_dict_path}\")\n",
    "    _dict = Dictionary.load(_dict_path)\n",
    "    for code in codes:\n",
    "        _dict.add_symbol(MultilingualTripletDataset.LANG_TAG_TEMPLATE.format(code))\n",
    "    _dict.add_symbol('<mask>')\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_list_filename = '/mnt/raid5/siqi/checkpoints/pretrained/mbart50.ft.n1/ML50_langs.txt'\n",
    "vocab_filename = '/mnt/raid5/siqi/checkpoints/pretrained/mbart50.ft.n1/dict.txt'\n",
    "phone_vocab_filename = '/mnt/raid5/siqi/datasets/covost2/phone_dict.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = MultilingualTripletDataset.get_lang_codes(lang_list_filename)\n",
    "dict = load_dict(vocab_filename)\n",
    "with open(phone_vocab_filename, 'r') as r:\n",
    "    phone_list = [l.strip() for l in r.readlines() if l.strip() != '']\n",
    "    phone_dict = {l: idx + 1 for idx, l in enumerate(phone_list)} # leave 0 as blank\n",
    "    phone_list = ['|'] + phone_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.src_dict = task.tgt_dict = dict\n",
    "task.phone_dict = phone_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.w2v2_model_path = '/mnt/raid5/siqi/checkpoints/pretrained/xlsr2_300m.pt'\n",
    "args.mbart50_dir = '/mnt/raid5/siqi/checkpoints/pretrained/mbart50.ft.n1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = W2V2Transformer.build_model(args, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path_1 = '/mnt/raid5/siqi/checkpoints/xlsr_phone_mbart_de_zh_ctc_text/checkpoint_best.pt'\n",
    "ckpt_path_2 = '/mnt/raid5/siqi/checkpoints/xlsr_phone_mbart_zh_ctc_text/checkpoint_best.pt'\n",
    "ckpt_path_3 = '/mnt/raid5/siqi/checkpoints/xlsr_phone_mbart_de_zh/checkpoint_best.pt'\n",
    "ckpt_path_4 = '/mnt/raid5/siqi/checkpoints/xlsr_mbart_n1/checkpoint_best.pt'\n",
    "# ckpt_path = '/mnt/raid5/siqi/checkpoints/xlsr_mbart_n1/checkpoint_best.pt'\n",
    "ckpt_1 = load_checkpoint_to_cpu(ckpt_path_1)\n",
    "ckpt_2 = load_checkpoint_to_cpu(ckpt_path_2)\n",
    "ckpt_3 = load_checkpoint_to_cpu(ckpt_path_3)\n",
    "ckpt_4 = load_checkpoint_to_cpu(ckpt_path_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(ckpt_4[\"model\"], strict=False)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare parallel De and Zh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/wmt21-dense-24-wide-en-x\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/wmt21-dense-24-wide-en-x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_st_de_en = load_df_from_tsv('/mnt/raid5/siqi/datasets/covost2/de/test_st_de_en.tsv')\n",
    "en_ref = test_st_de_en['tgt_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "zh_text = []\n",
    "for idx in tqdm(range(0, len(en_ref), batch_size)):\n",
    "    inputs = tokenizer(en_ref[idx : idx + batch_size], return_tensors='pt', padding=True).to(device)\n",
    "    generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.get_lang_id(\"zh\"))\n",
    "    zh_text.extend(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('resources/zh_text.npy', zh_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_text = np.load('resources/zh_text.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate parallel audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_tts.inference import AutoProcessor\n",
    "from tensorflow_tts.inference import TFAutoModel\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"tensorspeech/tts-tacotron2-baker-ch\")\n",
    "tacotron2 = TFAutoModel.from_pretrained(\"tensorspeech/tts-tacotron2-baker-ch\")\n",
    "mb_melgan = TFAutoModel.from_pretrained(\"tensorspeech/tts-mb_melgan-baker-ch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"这是一个开源的端到端中文语音合成系统\"\n",
    "\n",
    "input_ids = processor.text_to_sequence(text, inference=True)\n",
    "\n",
    "# tacotron2 inference (text-to-mel)\n",
    "decoder_output, mel_outputs, stop_token_prediction, alignment_history = tacotron2.inference(\n",
    "    input_ids=tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),\n",
    "    input_lengths=tf.convert_to_tensor([len(input_ids)], tf.int32),\n",
    "    speaker_ids=tf.convert_to_tensor([0], dtype=tf.int32),\n",
    ")\n",
    "\n",
    "# melgan inference (mel-to-wav)\n",
    "audio = mb_melgan.inference(mel_outputs)[0, :, 0]\n",
    "\n",
    "# save to file\n",
    "sf.write('./test.wav', audio, 16000, \"PCM_16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "for zh_utt, id in zip(tqdm(results[:n_sample]), test_st_de_en['id']):\n",
    "    tts = gTTS(zh_utt, lang='zh')\n",
    "    tts.save('/mnt/raid5/siqi/datasets/covost2/de/16kHz_zh/{}.mp3'.format(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample\n",
    "for id in tqdm(test_st_de_en['id'][:n_sample]):\n",
    "    path = '/mnt/raid5/siqi/datasets/covost2/de/16kHz_zh/{}.mp3'.format(id)\n",
    "    wave = torchaudio.load(path)[0]\n",
    "    torchaudio.save('/mnt/raid5/siqi/datasets/covost2/de/16kHz_zh/{}.wav'.format(id), wave, sample_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_audios = []\n",
    "for audio in test_st_de_en['audio']:\n",
    "    de_audios.append(os.path.join('/mnt/raid5/siqi/datasets/covost2/de/16kHz', audio))\n",
    "zh_audios = []\n",
    "for audio in test_st_de_en['audio'][:n_sample]:\n",
    "    zh_audios.append(os.path.join('/mnt/raid5/siqi/datasets/covost2/de/16kHz_zh', audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_de_audios = []\n",
    "for audio in test_st_de_en['audio'][-n_sample:]:\n",
    "    neg_de_audios.append(os.path.join('/mnt/raid5/siqi/datasets/covost2/de/16kHz', audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(path):\n",
    "    source = get_features_or_waveform(\n",
    "        path,\n",
    "        need_waveform=True,\n",
    "        sample_rate=16000,\n",
    "    )\n",
    "    source = th.from_numpy(source).float()\n",
    "    src_tokens = source.unsqueeze(0).to(device)\n",
    "    src_lengths = th.LongTensor([source.size(0)]).to(device)\n",
    "    speech_encoder_out = model.encoder.forward_speech(src_tokens, src_lengths)\n",
    "    return speech_encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_ft_per_layer = [[] for _ in range(24)]\n",
    "zh_ft_per_layer = [[] for _ in range(24)]\n",
    "neg_de_ft_per_layer = [[] for _ in range(24)]\n",
    "with th.no_grad():\n",
    "    for de_audio, zh_audio, neg_de_audio in zip(de_audios, tqdm(zh_audios), neg_de_audios):\n",
    "        de_res = compute(de_audio)\n",
    "        zh_res = compute(zh_audio)\n",
    "        neg_de_res = compute(neg_de_audio)\n",
    "        for idx in range(24):\n",
    "            de_ft_per_layer[idx].append(de_res['layer_results'][idx].squeeze().amax(dim=0))\n",
    "            zh_ft_per_layer[idx].append(zh_res['layer_results'][idx].squeeze().amax(dim=0))\n",
    "            neg_de_ft_per_layer[idx].append(neg_de_res['layer_results'][idx].squeeze().amax(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_per_layer = []\n",
    "neg_dist_per_layer = []\n",
    "for layer_idx in range(24):\n",
    "    dists = []\n",
    "    neg_dists = []\n",
    "    for ex_idx in range(n_sample):\n",
    "        l2_dist = (de_ft_per_layer[layer_idx][ex_idx] - zh_ft_per_layer[layer_idx][ex_idx]).norm() / zh_ft_per_layer[layer_idx][ex_idx].norm()\n",
    "        neg_l2_dist = (neg_de_ft_per_layer[layer_idx][ex_idx] - zh_ft_per_layer[layer_idx][ex_idx]).norm() / zh_ft_per_layer[layer_idx][ex_idx].norm()\n",
    "        # cos_dist = F.cosine_similarity(de_ft_per_layer[layer_idx][ex_idx], zh_ft_per_layer[layer_idx][ex_idx], dim=0)\n",
    "        # neg_cos_dist = F.cosine_similarity(neg_de_ft_per_layer[layer_idx][ex_idx], zh_ft_per_layer[layer_idx][ex_idx], dim=0)\n",
    "        dists.append(l2_dist.item())\n",
    "        neg_dists.append(neg_l2_dist.item())\n",
    "    dist_per_layer.append(dists)\n",
    "    neg_dist_per_layer.append(neg_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_dist_per_layer = []\n",
    "avg_neg_dist_per_layer = []\n",
    "for layer_idx in range(24):\n",
    "    avg_dist = sum(dist_per_layer[layer_idx]) / n_sample\n",
    "    avg_neg_dist = sum(neg_dist_per_layer[layer_idx]) / n_sample\n",
    "    avg_dist_per_layer.append(avg_dist)\n",
    "    avg_neg_dist_per_layer.append(avg_neg_dist)\n",
    "plt.plot(avg_dist_per_layer, label='mean dist')\n",
    "plt.plot(avg_neg_dist_per_layer, label='mean neg dist')\n",
    "plt.xlabel('layer')\n",
    "plt.ylabel('mean dist')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_df = pd.DataFrame(columns=['dist', 'neg_dist', 'layer'])\n",
    "dists = []\n",
    "neg_dists = []\n",
    "for idx in range(0, 24, 4):\n",
    "    dists.extend(dist_per_layer[idx])\n",
    "    neg_dists.extend(neg_dist_per_layer[idx])\n",
    "sns_df['dist'] = dists\n",
    "sns_df['neg_dist'] = neg_dists\n",
    "layer_indices = [idx for idx in range(0, 24, 4) for _ in range(n_sample)]\n",
    "sns_df['layer'] = layer_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(sns_df, x='dist', hue='layer', kind='kde')\n",
    "plt.xlim(0, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(sns_df, x='neg_dist', hue='layer', kind='kde')\n",
    "plt.xlim(0, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio retrieve test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_de_ft_per_layer = [[] for _ in range(24)]\n",
    "with th.no_grad():\n",
    "    for de_audio in tqdm(de_audios):\n",
    "        de_res = compute(de_audio)\n",
    "        for idx in range(24):\n",
    "            full_de_ft_per_layer[idx].append(de_res['layer_results'][idx].squeeze().amax(dim=0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx in range(24):\n",
    "    full_de_ft_per_layer[layer_idx] = np.array([t.numpy() for t in full_de_ft_per_layer[layer_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "accs = []\n",
    "for layer_idx in range(24):\n",
    "    index = faiss.IndexFlatL2(1024)\n",
    "    index.add(np.array(full_de_ft_per_layer[layer_idx]))\n",
    "    \n",
    "    qry = np.array([t.cpu().numpy() for t in zh_ft_per_layer[layer_idx]])\n",
    "    D, I = index.search(qry, 100)\n",
    "    acc = sum([idx in I[idx] for idx in range(len(zh_ft_per_layer[layer_idx]))]) / len(zh_ft_per_layer[layer_idx])\n",
    "    accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(accs)\n",
    "plt.xlabel('layer')\n",
    "plt.ylabel('top-100 acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(n_sample):\n",
    "    print(zh_text[idx], en_ref[I[idx, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CER comparison of De+Zh and Zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lang_list_filename, 'r') as r:\n",
    "    lang_codes = [line.strip() for line in r.readlines() if line.strip() != \"\"]\n",
    "lang2langcode = {code[:2] : code for code in lang_codes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm_config = SentencepieceConfig(sentencepiece_model='/mnt/raid0/siqi/checkpoints/pretrained/mbart50.ft.n1/sentence.bpe.model')\n",
    "spm = SentencepieceBPE(spm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/mnt/raid5/siqi/datasets/covost2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df_from_tsv('/mnt/raid5/siqi/datasets/covost2/zh-CN/dev_st_zh-CN_en_phone.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    preds = []\n",
    "    refs = df['src_text'].tolist()\n",
    "    cers = []\n",
    "    for idx in tqdm(range(len(df))):\n",
    "        source = get_features_or_waveform(\n",
    "            os.path.join(data_root, 'zh-CN', '16kHz', df.iloc[idx]['audio']),\n",
    "            need_waveform=True,\n",
    "            sample_rate=16000,\n",
    "        )\n",
    "        source = th.from_numpy(source).float()\n",
    "\n",
    "        lang_tag = \"<lang:{}>\".format(lang2langcode['zh'])\n",
    "        src_lang_tag_idx = dict.index(lang_tag)\n",
    "\n",
    "        src_tokens = source.unsqueeze(0).to(device)\n",
    "        src_lengths = th.LongTensor([source.size(0)]).to(device)\n",
    "        src_lang_tag_indices = th.LongTensor([src_lang_tag_idx]).unsqueeze(-1).to(device)\n",
    "\n",
    "        encoder_out = model.forward_encoder(src_tokens, src_lengths, src_lang_tag_indices=src_lang_tag_indices)\n",
    "\n",
    "        pred = spm.decode(' '.join([dict[tok] for tok in encoder_out.text_logp.argmax(dim=-1).squeeze().unique_consecutive() if dict[tok] not in ['<mask>', '<lang:zh_CN>', '</s>']]))\n",
    "        # print(spm.encode(df.iloc[idx]['src_text']))\n",
    "        ref = df.iloc[idx]['src_text']\n",
    "\n",
    "        cer = editdistance.eval(pred, ref) / len(ref)\n",
    "\n",
    "        preds.append(pred)\n",
    "        cers.append(cer)\n",
    "preds = np.array(preds)\n",
    "cers = np.array(cers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_preds = preds\n",
    "zh_cers = cers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zh_preds = preds\n",
    "de_zh_cers = cers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_df = pd.DataFrame(columns=['cer', 'label'])\n",
    "sns_df['cer'] = np.concatenate([de_zh_cers, zh_cers])\n",
    "sns_df['label'] = ['de+zh'] * len(de_zh_cers) + ['zh'] * len(zh_cers)\n",
    "sns.displot(sns_df, x='cer', hue='label', kind='kde')\n",
    "plt.xlabel('CER')\n",
    "plt.xlim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_zh_preds = zh_preds\n",
    "dev_zh_cers = zh_cers\n",
    "dev_de_zh_preds = de_zh_preds\n",
    "dev_de_zh_cers = de_zh_cers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS on features produced by De+Zh and Zh for each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_st_zh_en = load_df_from_tsv('/mnt/raid5/siqi/datasets/covost2/zh-CN/train_st_zh-CN_en.tsv')\n",
    "dev_st_zh_en = load_df_from_tsv('/mnt/raid5/siqi/datasets/covost2/zh-CN/dev_st_zh-CN_en.tsv')\n",
    "test_st_zh_en = load_df_from_tsv('/mnt/raid5/siqi/datasets/covost2/zh-CN/test_st_zh-CN_en.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zh_audios = []\n",
    "for audio in train_st_zh_en['audio']:\n",
    "    train_zh_audios.append(os.path.join('/mnt/raid5/siqi/datasets/covost2/zh-CN/16kHz', audio))\n",
    "dev_zh_audios = []\n",
    "for audio in dev_st_zh_en['audio']:\n",
    "    dev_zh_audios.append(os.path.join('/mnt/raid5/siqi/datasets/covost2/zh-CN/16kHz', audio))\n",
    "test_zh_audios = []\n",
    "for audio in test_st_zh_en['audio']:\n",
    "    test_zh_audios.append(os.path.join('/mnt/raid5/siqi/datasets/covost2/zh-CN/16kHz', audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = '/mnt/raid5/siqi/analysis/zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_ft(audios, n_frames, name, dirname, batch_size=1024):\n",
    "    ft_per_layer = [[] for _ in range(24)]\n",
    "    batch_idx = 0\n",
    "    with th.no_grad():\n",
    "        sum_len = 0\n",
    "        for idx, audio in enumerate(tqdm(audios, desc=name)):\n",
    "            res = compute(audio)\n",
    "            cur_len = res['layer_results'][0].size(0)\n",
    "\n",
    "            if sum_len + cur_len > batch_size:\n",
    "                for layer_idx in range(24):\n",
    "                    th.save(ft_per_layer[layer_idx], os.path.join(dirname, '{}_layer_{}_batch_{}.pt'.format(name, layer_idx, batch_idx)))\n",
    "                    ft_per_layer[layer_idx] = []\n",
    "                batch_idx += 1\n",
    "                sum_len = 0\n",
    "            \n",
    "            for layer_idx in range(24):\n",
    "                ft_per_layer[layer_idx].append(res['layer_results'][layer_idx].cpu().squeeze(dim=1))\n",
    "            sum_len += cur_len\n",
    "        \n",
    "        if len(ft_per_layer[0]) > 0:\n",
    "            for layer_idx in range(24):\n",
    "                th.save(ft_per_layer[layer_idx], os.path.join(dirname, '{}_layer_{}_batch_{}.pt'.format(name, layer_idx, batch_idx)))\n",
    "                ft_per_layer[layer_idx] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precompute_ft(test_zh_audios, test_st_zh_en['n_frames'], 'test', res_dir, batch_size=8192)\n",
    "precompute_ft(dev_zh_audios, dev_st_zh_en['n_frames'], 'dev', res_dir, batch_size=8192)\n",
    "precompute_ft(train_zh_audios, train_st_zh_en['n_frames'], 'train', res_dir, batch_size=8192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce POS labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu(gpu_id=int(device[-1]))\n",
    "nlp = spacy.load(\"zh_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zh_text = train_st_zh_en['src_text'].tolist()\n",
    "dev_zh_text = dev_st_zh_en['src_text'].tolist()\n",
    "test_zh_text = test_st_zh_en['src_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_tag(texts, n_frames, name, dirname):\n",
    "    tags = []\n",
    "    for text in tqdm(texts, desc=name):\n",
    "        cur_tag = []\n",
    "        for token in nlp(text):\n",
    "            cur_tag.append(token.tag_)\n",
    "        tags.append(cur_tag)\n",
    "    th.save(tags, os.path.join(dirname, '{}_tag.pt'.format(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precompute_tag(train_zh_text, train_st_zh_en['n_frames'], 'train', res_dir)\n",
    "precompute_tag(dev_zh_text, dev_st_zh_en['n_frames'], 'dev', res_dir)\n",
    "precompute_tag(test_zh_text, test_st_zh_en['n_frames'], 'test', res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC on POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather tags\n",
    "train_tags = th.load(os.path.join('/mnt/raid5/siqi/analysis/de_zh', 'train_tag.pt'))\n",
    "dev_tags = th.load(os.path.join('/mnt/raid5/siqi/analysis/de_zh', 'dev_tag.pt'))\n",
    "test_tags = th.load(os.path.join('/mnt/raid5/siqi/analysis/de_zh', 'test_tag.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDdataset:\n",
    "    def __init__(self, dirname, split, layer, labels):\n",
    "        self.dirname = dirname\n",
    "        files = os.listdir(dirname)\n",
    "        self.batches = {}\n",
    "        pattern = re.compile('{}\\_layer\\_{}\\_batch\\_(\\d+)\\.pt'.format(split, layer))\n",
    "        for fn in files:\n",
    "            res = pattern.match(fn)\n",
    "            if res is not None:\n",
    "                batch_idx = int(res.group(1))\n",
    "                self.batches[batch_idx] = fn\n",
    "        self.n_batch = len(self.batches)\n",
    "        cum_idx = 0\n",
    "        for batch_idx in range(self.n_batch):\n",
    "            batch_size = len(th.load(os.path.join(dirname, self.batches[batch_idx])))\n",
    "            self.batches[batch_idx] = (self.batches[batch_idx], labels[cum_idx : cum_idx + batch_size])\n",
    "            cum_idx += batch_size\n",
    "\n",
    "    def __getitem__(self, batch_idx):\n",
    "        fn, labels = self.batches[batch_idx]\n",
    "        features = th.load(os.path.join(self.dirname, fn))\n",
    "        return features, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_data_dir = '/mnt/raid5/siqi/analysis/zh'\n",
    "zh_train = [POSDdataset(zh_data_dir, 'train', layer, train_tags) for layer in tqdm(range(24))]\n",
    "zh_dev = [POSDdataset(zh_data_dir, 'dev', layer, dev_tags) for layer in tqdm(range(24))]\n",
    "zh_test = [POSDdataset(zh_data_dir, 'test', layer, test_tags) for layer in tqdm(range(24))]\n",
    "th.save([zh_train, zh_dev, zh_test], 'zh_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zh_data_dir = '/mnt/raid5/siqi/analysis/de_zh'\n",
    "de_zh_train = [POSDdataset(de_zh_data_dir, 'train', layer, train_tags) for layer in tqdm(range(24))]\n",
    "de_zh_dev = [POSDdataset(de_zh_data_dir, 'dev', layer, dev_tags) for layer in tqdm(range(24))]\n",
    "de_zh_test = [POSDdataset(de_zh_data_dir, 'test', layer, test_tags) for layer in tqdm(range(24))]\n",
    "th.save([de_zh_train, de_zh_dev, de_zh_test], 'de_zh_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_train, zh_dev, zh_test = th.load('zh_dataset.pt')\n",
    "de_zh_train, de_zh_dev, de_zh_test = th.load('de_zh_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.linear = nn.Linear(1024, n_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.linear(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"zh_core_web_trf\")\n",
    "idx2label = list(nlp.components[1][1].labels)\n",
    "label2idx = {label : idx for idx, label in enumerate(idx2label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(features):\n",
    "    max_len = max(ft.size(0) for ft in features)\n",
    "    output = th.zeros(len(features), max_len, 1024)\n",
    "    for idx, ft in enumerate(features):\n",
    "        output[idx][:ft.size(0)] = ft\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(features, targets):\n",
    "    input_lengths = th.tensor([ft.size(0) for ft in features])\n",
    "    target_lengths = th.tensor([len(tgt) for tgt in targets])\n",
    "    features = collate(features).transpose(0, 1)\n",
    "    targets = [[label2idx[tok] for tok in tgt]  for tgt in targets]\n",
    "    targets = th.cat([th.tensor(tgt) for tgt in targets], dim=0) + 1\n",
    "\n",
    "    return features.to(device), targets, input_lengths, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_name in [\"zh\"]:\n",
    "    train_datasets = de_zh_train if data_name == \"de_zh\" else zh_train\n",
    "    dev_datasets = de_zh_dev if data_name == \"de_zh\" else zh_dev\n",
    "    for layer_idx in range(0, 24, 2):\n",
    "        with wandb.init(project=\"ST\", entity=\"owaski\", name=\"{}_layer_{}\".format(data_name, layer_idx)):\n",
    "            predictor = Predictor(len(idx2label) + 1).to(device)\n",
    "            optimizer = th.optim.Adam(predictor.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "            train_dataset = train_datasets[layer_idx]\n",
    "            dev_dataset = dev_datasets[layer_idx]\n",
    "\n",
    "            collate_fn = lambda x: (sum([a for a, b in x], []), sum([b for a, b in x], []))\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "            dev_dataloader = DataLoader(dev_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "            n_epoch = 100\n",
    "            iterator = tqdm(range(n_epoch))\n",
    "            for epoch_idx in iterator:\n",
    "                predictor.train()\n",
    "                sum_loss = 0.\n",
    "                n_data = 0\n",
    "                for features, targets in train_dataloader:\n",
    "                    features, targets, input_lengths, target_lengths = convert(features, targets)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    logps = predictor(features)\n",
    "                    loss = F.ctc_loss(logps, targets, input_lengths, target_lengths, blank=0, reduction='mean')\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    sum_loss += loss.item() * len(input_lengths)\n",
    "                    n_data += len(input_lengths)\n",
    "                train_loss = sum_loss / n_data\n",
    "\n",
    "                wandb.log({\"train_loss\": train_loss}, step=epoch_idx)\n",
    "                \n",
    "                if (epoch_idx + 1) % 3 == 0:\n",
    "                    predictor.eval()\n",
    "                    sum_loss = 0.\n",
    "                    n_data = 0\n",
    "                    with th.no_grad():\n",
    "                        for features, targets in dev_dataloader:\n",
    "                            features, targets, input_lengths, target_lengths = convert(features, targets)\n",
    "                            logps = predictor(features)\n",
    "                            loss = F.ctc_loss(logps, targets, input_lengths, target_lengths, blank=0, reduction='mean')\n",
    "                            sum_loss += loss.item() * len(input_lengths)\n",
    "                            n_data += len(input_lengths)\n",
    "                    eval_loss = sum_loss / n_data\n",
    "                    \n",
    "                    wandb.log({\"eval_loss\": eval_loss}, step=epoch_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probing attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(ckpt_1[\"model\"], strict=False)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textgrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1_diag = []\n",
    "for idx in tqdm(range(100)):\n",
    "    try:\n",
    "        audio_id = test_st_zh_en['id'][idx]\n",
    "        audio_path = test_zh_audios[idx]\n",
    "        text = test_st_zh_en['src_text'][idx]\n",
    "        grids = textgrids.TextGrid('/mnt/raid5/siqi/datasets/covost2/zh-CN/16kHz_test/align/{}.TextGrid'.format(audio_id))\n",
    "\n",
    "        # waveform = torchaudio.load(audio_path)\n",
    "        # plt.figure(dpi=100, figsize=(10, 5))\n",
    "        # plt.plot(th.arange(waveform[0][0].size(0)) / 16000, waveform[0][0])\n",
    "\n",
    "        duration = torchaudio.info(audio_path).num_frames / 16000\n",
    "        intervals = th.tensor([(grid.xmin, grid.xmax) for grid in grids['words'] if grid.text != '']) / duration\n",
    "        tokens = [grid.text for grid in grids['words'] if grid.text != '']\n",
    "\n",
    "        with th.no_grad():\n",
    "            speech_encoder_out = compute(audio_path)\n",
    "\n",
    "        # per layer attn\n",
    "        # os.makedirs('figures/{}'.format(idx), exist_ok=True)\n",
    "        # for layer in tqdm(range(24)):\n",
    "        #     n_unit = speech_encoder_out['layer_results'][layer][0].size(0)\n",
    "        #     unit_intervals = (intervals * n_unit).long()\n",
    "        #     attn_matrix = th.zeros(intervals.size(0), intervals.size(0))\n",
    "        #     for i, (xmin, xmax) in enumerate(unit_intervals):\n",
    "        #         for j, (ymin, ymax) in enumerate(unit_intervals):\n",
    "        #             score = speech_encoder_out['layer_results'][layer][0][xmin : xmax + 1, ymin : ymax + 1].sum(dim=1).mean(dim=0)\n",
    "        #             attn_matrix[i, j] = score\n",
    "\n",
    "        #     plt.figure(dpi=100, figsize=(12, 10))\n",
    "        #     sns.heatmap(attn_matrix, xticklabels=tokens, yticklabels=tokens, vmin=0, vmax=1.0, annot=True, cmap=\"YlGnBu\")\n",
    "        #     plt.savefig('figures/{}/de_zh_no_ctc_layer_{}.png'.format(idx, layer))\n",
    "        #     plt.close()\n",
    "\n",
    "        # mean attn over layers\n",
    "        os.makedirs('figures/{}'.format(idx), exist_ok=True)\n",
    "        attn_matrix = th.zeros(intervals.size(0), intervals.size(0))\n",
    "        for layer in range(24):\n",
    "            n_unit = speech_encoder_out['layer_results'][layer][0].size(0)\n",
    "            unit_intervals = (intervals * n_unit).long()\n",
    "            for i, (xmin, xmax) in enumerate(unit_intervals):\n",
    "                for j, (ymin, ymax) in enumerate(unit_intervals):\n",
    "                    score = speech_encoder_out['layer_results'][layer][0][xmin : xmax + 1, ymin : ymax + 1].sum(dim=1).mean(dim=0)\n",
    "                    attn_matrix[i, j] += score.item()\n",
    "        n1_diag.append(attn_matrix.diag().sum().item())\n",
    "        # plt.figure(dpi=100, figsize=(12, 10))\n",
    "        # sns.heatmap(attn_matrix, xticklabels=tokens, yticklabels=tokens, annot=True, cmap=\"YlGnBu\")\n",
    "    except:\n",
    "        print('skip', idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de_zh_ctc_text_diag = th.tensor(de_zh_ctc_text_diag)\n",
    "# zh_ctc_text_diag = th.tensor(zh_ctc_text_diag)\n",
    "# de_zh_diag = th.tensor(de_zh_diag)\n",
    "n1_diag = th.tensor(n1_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame(columns=['diag', 'model'])\n",
    "plot_df['diag'] = th.cat([de_zh_ctc_text_diag, zh_ctc_text_diag, de_zh_diag, n1_diag])\n",
    "plot_df['model'] = ['de_zh_ctc_text'] * de_zh_ctc_text_diag.size(0) + ['zh_ctc_text'] * zh_ctc_text_diag.size(0) + ['de_zh'] * de_zh_diag.size(0) + ['n1'] * n1_diag.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(plot_df, x='diag', hue='model', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_st_de_en = load_df_from_tsv('/mnt/raid5/siqi/datasets/covost2/de/test_st_de_en.tsv')\n",
    "test_de_audios = []\n",
    "for audio in test_st_de_en['audio']:\n",
    "    test_de_audios.append(os.path.join('/mnt/raid5/siqi/datasets/covost2/de/16kHz', audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "audio_id = test_st_de_en['id'][idx]\n",
    "audio_path = test_de_audios[idx]\n",
    "text = test_st_de_en['src_text'][idx]\n",
    "print(text)\n",
    "grids = textgrids.TextGrid('/mnt/raid5/siqi/datasets/covost2/de/16kHz_test/align/{}.TextGrid'.format(audio_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = torchaudio.info(audio_path).num_frames / 16000\n",
    "intervals = th.tensor([(grid.xmin, grid.xmax) for grid in grids['words'] if grid.text != '']) / duration\n",
    "tokens = [grid.text for grid in grids['words'] if grid.text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "    speech_encoder_out = compute(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per layer attn\n",
    "# os.makedirs('figures/{}'.format(idx), exist_ok=True)\n",
    "for layer in tqdm(range(24)):\n",
    "    n_unit = speech_encoder_out['layer_results'][layer][0].size(0)\n",
    "    unit_intervals = (intervals * n_unit).long()\n",
    "    attn_matrix = th.zeros(intervals.size(0), intervals.size(0))\n",
    "    for i, (xmin, xmax) in enumerate(unit_intervals):\n",
    "        for j, (ymin, ymax) in enumerate(unit_intervals):\n",
    "            score = speech_encoder_out['layer_results'][layer][0][xmin : xmax + 1, ymin : ymax + 1].sum(dim=1).mean(dim=0)\n",
    "            attn_matrix[i, j] = score\n",
    "\n",
    "    plt.figure(dpi=100, figsize=(12, 10))\n",
    "    sns.heatmap(attn_matrix, xticklabels=tokens, yticklabels=tokens, vmin=0, vmax=1.0, annot=True, cmap=\"YlGnBu\")\n",
    "    # plt.savefig('figures/{}/de_zh_no_ctc_layer_{}.png'.format(idx, layer))\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean attn over layers\n",
    "# os.makedirs('figures/{}'.format(idx), exist_ok=True)\n",
    "attn_matrix = th.zeros(intervals.size(0), intervals.size(0))\n",
    "for layer in tqdm(range(24)):\n",
    "    n_unit = speech_encoder_out['layer_results'][layer][0].size(0)\n",
    "    unit_intervals = (intervals * n_unit).long()\n",
    "    for i, (xmin, xmax) in enumerate(unit_intervals):\n",
    "        for j, (ymin, ymax) in enumerate(unit_intervals):\n",
    "            score = speech_encoder_out['layer_results'][layer][0][xmin : xmax + 1, ymin : ymax + 1].sum(dim=1).mean(dim=0)\n",
    "            attn_matrix[i, j] += score.item()\n",
    "plt.figure(dpi=100, figsize=(12, 10))\n",
    "sns.heatmap(attn_matrix, xticklabels=tokens, yticklabels=tokens, annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_st_de_en = load_df_from_tsv('/mnt/raid5/siqi/datasets/covost2/de/test_st_de_en.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, src_text in zip(test_st_de_en['id'][:100], test_st_de_en['src_text']):\n",
    "    os.system('cp /mnt/raid5/siqi/datasets/covost2/de/16kHz/{}.wav /mnt/raid5/siqi/datasets/covost2/de/16kHz_test/{}.wav'.format(id, id))\n",
    "    with open('/mnt/raid5/siqi/datasets/covost2/de/16kHz_test/{}.txt'.format(id), 'w') as w:\n",
    "        w.write(src_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a68cd4a7ef308fe1097ac63f4e85e531ee114d7b566f56457f8a7bdc63a1a0bb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('st')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
