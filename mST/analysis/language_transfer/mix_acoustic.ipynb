{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bert_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/siqiouyang/work/projects/mST/mST/analysis/language_transfer/mix_acoustic.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blei1/home/siqiouyang/work/projects/mST/mST/analysis/language_transfer/mix_acoustic.ipynb#ch0000000vscode-remote?line=24'>25</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtextgrids\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blei1/home/siqiouyang/work/projects/mST/mST/analysis/language_transfer/mix_acoustic.ipynb#ch0000000vscode-remote?line=25'>26</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfaiss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blei1/home/siqiouyang/work/projects/mST/mST/analysis/language_transfer/mix_acoustic.ipynb#ch0000000vscode-remote?line=27'>28</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbert_score\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blei1/home/siqiouyang/work/projects/mST/mST/analysis/language_transfer/mix_acoustic.ipynb#ch0000000vscode-remote?line=29'>30</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blei1/home/siqiouyang/work/projects/mST/mST/analysis/language_transfer/mix_acoustic.ipynb#ch0000000vscode-remote?line=31'>32</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfairseq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dictionary\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bert_score'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from argparse import Namespace\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "\n",
    "import IPython\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sentencepiece as spm\n",
    "\n",
    "import textgrids\n",
    "import faiss\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from fairseq.data import Dictionary\n",
    "from fairseq.models.mST.w2v2_phone_transformer import W2V2Transformer\n",
    "from fairseq.data.audio.multilingual_triplet_v2_phone_dataset import (\n",
    "    MultilingualTripletDataConfig,\n",
    "    MultilingualTripletDataset,\n",
    "    MultilingualTripletDatasetCreator\n",
    ")\n",
    "from fairseq.data.audio.speech_to_text_dataset import get_features_or_waveform\n",
    "from examples.speech_to_text.data_utils import load_df_from_tsv, save_df_to_tsv\n",
    "from fairseq.checkpoint_utils import load_checkpoint_to_cpu\n",
    "from fairseq.data.encoders.sentencepiece_bpe import SentencepieceBPE, SentencepieceConfig\n",
    "\n",
    "from fairseq.models.mST.w2v2_phone_transformer import W2V2Transformer\n",
    "from fairseq.data.audio.multilingual_triplet_v2_phone_dataset import (\n",
    "    MultilingualTripletDataConfig,\n",
    "    MultilingualTripletDataset,\n",
    "    MultilingualTripletDatasetCreator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE').to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process CV-9.0 Zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_zh_root = '/mnt/data/siqiouyang/datasets/cv-corpus-9.0-2022-04-27/zh-CN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_zh_df = load_df_from_tsv(os.path.join(cv_zh_root, 'validated.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_zh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in tqdm(cv_zh_df['path']):\n",
    "#     audio_path = os.path.join(cv_zh_root, 'clips/{}'.format(path))\n",
    "#     waveform, sample_rate = torchaudio.load(audio_path)\n",
    "#     resampled_waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
    "#     torchaudio.save(os.path.join(cv_zh_root, '16kHz/{}.wav'.format(path[:-4])), resampled_waveform, sample_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = cv_zh_df['sentence'].tolist()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/siqiouyang/work/projects/WMSeg/zh.txt', 'w') as w:\n",
    "#     w.write('\\n'.join(sentences).replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = re.compile(\",|\\?|・|\\u3002|\\uff1f|\\uff01|\\uff0c|\\u3001|\\uff1b|\\uff1a|\\u201c|\\u201d|\\u2018|\\u2019|\\uff08|\\uff09|\\u300a|\\u300b|\\u3008|\\u3009|\\u3010|\\u3011|\\u300e|\\u300f|\\u300c|\\u300d|\\ufe43|\\ufe44|\\u3014|\\u3015|\\u2026|\\u2014|\\uff5e|\\ufe4f|\\uffe5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = []\n",
    "with open('/home/siqiouyang/work/projects/WMSeg/cv9_zh.txt.tok', 'r') as r:\n",
    "    for line in r.readlines():\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            line = punc.sub(' ', line)\n",
    "            tokens = [tok for tok in line.split(' ') if tok != '']\n",
    "            tokenized_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path, tokens in zip(cv_zh_df['path'], tokenized_sentences):\n",
    "#     with open(os.path.join(cv_zh_root, '16kHz', '{}.txt'.format(path[:-4])), 'w') as w:\n",
    "#         w.write(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tokenized_sentences = []\n",
    "final_segmentations = []\n",
    "final_audio_paths = []\n",
    "for i, path in enumerate(tqdm(cv_zh_df['path'])):\n",
    "    id = path[:-4]\n",
    "    grid_path = os.path.join(cv_zh_root, '16kHz/align_wmseg/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        filtered_grid = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        if len(filtered_grid) != len(tokenized_sentences[i]):\n",
    "            print(filtered_grid, tokenized_sentences[i], sep='\\n', end='\\n' + '-'*10 + '\\n')\n",
    "\n",
    "        interval = np.array([(word.xmin, word.xmax) for word in filtered_grid])\n",
    "        audio_path = os.path.join(cv_zh_root, '16kHz/{}.wav'.format(id))\n",
    "        info = torchaudio.info(audio_path)\n",
    "        duration = info.num_frames / info.sample_rate\n",
    "        interval = interval / duration\n",
    "        final_segmentations.append(interval)\n",
    "\n",
    "        final_audio_paths.append(audio_path)\n",
    "        final_tokenized_sentences.append(tokenized_sentences[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramDictionary:\n",
    "    def __init__(self, tokenized_sentences, segmentations, audio_paths, gram=3, sep=''):\n",
    "        self.tokenized_sentences = tokenized_sentences\n",
    "\n",
    "        self.id2tok = []\n",
    "        self.tok2id = {}\n",
    "        self.embs = []\n",
    "        self.waveforms = {}\n",
    "\n",
    "        assert len(tokenized_sentences) == len(segmentations) and len(segmentations) == len(audio_paths)\n",
    "\n",
    "        for tok_sent, seg, path in zip(tqdm(tokenized_sentences), segmentations, audio_paths):\n",
    "            waveform = torchaudio.load(path)[0][0]\n",
    "            len_wf = waveform.size(0)\n",
    "            assert len(tok_sent) == len(seg)\n",
    "            for l in range(1, gram + 1):\n",
    "                for idx in range(len(tok_sent) - l + 1):\n",
    "                    tok = sep.join(tok_sent[idx : idx + l])\n",
    "                    xmin, xmax = seg[idx][0], seg[idx + l - 1][1]\n",
    "                    \n",
    "                    tok_wf = waveform[int(xmin * len_wf) : int(xmax * len_wf)]\n",
    "                    if tok not in self.tok2id:\n",
    "                        self.tok2id[tok] = len(self.id2tok)\n",
    "                        self.id2tok.append(tok)\n",
    "                        self.waveforms[self.tok2id[tok]] = [tok_wf]\n",
    "                    else:\n",
    "                        self.waveforms[self.tok2id[tok]].append(tok_wf)\n",
    "\n",
    "        batch_size = 10000\n",
    "        for idx in tqdm(range(0, len(self.id2tok), batch_size)):\n",
    "            self.embs.append(model.encode(self.id2tok[idx : idx + batch_size]))\n",
    "        \n",
    "        self.embs = np.concatenate(self.embs, axis=0)\n",
    "        self.index = faiss.IndexFlatIP(self.embs.shape[-1])\n",
    "        self.index.add(self.embs)\n",
    "\n",
    "        self.id2tok = np.array(self.id2tok)\n",
    "\n",
    "    def search(self, x, k):\n",
    "        return self.index.search(x, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_zh_dict = NgramDictionary(final_tokenized_sentences, final_segmentations, final_audio_paths, gram=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_zh_dict = th.load('dict/zh-CN_dict.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Chinese Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/data/siqiouyang/datasets/covost2/zh-CN'\n",
    "df = load_df_from_tsv(os.path.join(root, 'train_st_zh-CN_en.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = []\n",
    "for id in df['id']:\n",
    "    audio_paths.append(os.path.join(root, '16kHz/{}.wav'.format(id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['src_text'].tolist()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/siqiouyang/work/projects/WMSeg/zh.txt', 'w') as w:\n",
    "#     w.write('\\n'.join(sentences).replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = []\n",
    "with open('/home/siqiouyang/work/projects/WMSeg/zh.txt.tok', 'r') as r:\n",
    "    for line in r.readlines():\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            line = punc.sub(' ', line)\n",
    "            tokens = [tok for tok in line.split(' ') if tok != '']\n",
    "            tokenized_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id, tokens in zip(df['id'], tokenized_sentences):\n",
    "#     with open(os.path.join(root, '16kHz', '{}.txt'.format(id)), 'w') as w:\n",
    "#         w.write(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfa to force align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_grids = []\n",
    "for i, id in enumerate(df['id']):\n",
    "    grid = textgrids.TextGrid(os.path.join(root, '16kHz/align_wmseg/{}.TextGrid'.format(id)))\n",
    "    filtered_grid = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "    if len(filtered_grid) != len(tokenized_sentences[i]):\n",
    "        print(filtered_grid, tokenized_sentences[i], sep='\\n', end='\\n' + '-'*10 + '\\n')\n",
    "\n",
    "    filtered_grids.append(filtered_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentations = []\n",
    "for grid, path in zip(filtered_grids, audio_paths):\n",
    "    interval = np.array([(word.xmin, word.xmax) for word in grid])\n",
    "    info = torchaudio.info(path)\n",
    "    duration = info.num_frames / info.sample_rate\n",
    "    interval = interval / duration\n",
    "    segmentations.append(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    def __init__(self, tokenized_sentences, segmentations, audio_paths):\n",
    "        self.tokenized_sentences = tokenized_sentences\n",
    "\n",
    "        self.id2tok = []\n",
    "        self.tok2id = {}\n",
    "        self.embs = []\n",
    "        self.waveforms = {}\n",
    "\n",
    "        assert len(tokenized_sentences) == len(segmentations) and len(segmentations) == len(audio_paths)\n",
    "\n",
    "        for tok_sent, seg, path in zip(tqdm(tokenized_sentences), segmentations, audio_paths):\n",
    "            waveform = torchaudio.load(path)[0][0]\n",
    "            len_wf = waveform.size(0)\n",
    "            assert len(tok_sent) == len(seg), print(tok_sent, seg)\n",
    "            for tok, (xmin, xmax) in zip(tok_sent, seg):\n",
    "                tok_wf = waveform[int(xmin * len_wf) : int(xmax * len_wf)]\n",
    "                if tok not in self.tok2id:\n",
    "                    self.tok2id[tok] = len(self.id2tok)\n",
    "                    self.id2tok.append(tok)\n",
    "                    self.waveforms[self.tok2id[tok]] = [tok_wf]\n",
    "                else:\n",
    "                    self.waveforms[self.tok2id[tok]].append(tok_wf)\n",
    "\n",
    "        batch_size = 10000\n",
    "        for idx in tqdm(range(0, len(self.id2tok), batch_size)):\n",
    "            self.embs.append(model.encode(self.id2tok[idx : idx + batch_size]))\n",
    "        \n",
    "        self.embs = np.concatenate(self.embs, axis=0)\n",
    "        self.index = faiss.IndexFlatIP(self.embs.shape[-1])\n",
    "        self.index.add(self.embs)\n",
    "\n",
    "        self.id2tok = np.array(self.id2tok)\n",
    "\n",
    "    def search(self, x, k):\n",
    "        return self.index.search(x, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zh_dict = Dictionary(tokenized_sentences, segmentations, audio_paths)\n",
    "zh_dict = NgramDictionary(tokenized_sentences, segmentations, audio_paths, gram=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build German Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/data/siqiouyang/datasets/covost2/de'\n",
    "df = load_df_from_tsv(os.path.join(root, 'train_st_de_en.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['src_text'].tolist()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_punctuation = punctuation + '„“”‚’«»ʿ‹›‘'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = []\n",
    "for sent in sentences:\n",
    "    sent = ''.join(c for c in sent if c not in punctuation)\n",
    "    tokens = [tok for tok in sent.split(' ') if tok != '']\n",
    "    tokenized_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfa to force align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_grids = []\n",
    "audio_paths = []\n",
    "filtered_tokenized_sentences = []\n",
    "for i, id in enumerate(tqdm(df['id'])):\n",
    "    grid_path = os.path.join(root, '16kHz/align/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        filtered_grid = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        if len(filtered_grid) != len(tokenized_sentences[i]):\n",
    "            # print(filtered_grid, tokenized_sentences[i], sep='\\n', end='\\n' + '-'*10 + '\\n')\n",
    "            continue\n",
    "        \n",
    "        filtered_grids.append(filtered_grid)\n",
    "        filtered_tokenized_sentences.append(tokenized_sentences[i])\n",
    "        audio_paths.append(os.path.join(root, '16kHz/{}.wav'.format(id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentations = []\n",
    "for grid, path in zip(filtered_grids, audio_paths):\n",
    "    interval = np.array([(word.xmin, word.xmax) for word in grid])\n",
    "    info = torchaudio.info(path)\n",
    "    duration = info.num_frames / info.sample_rate\n",
    "    interval = interval / duration\n",
    "    segmentations.append(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de_dict = Dictionary(filtered_tokenized_sentences, segmentations, audio_paths)\n",
    "de_dict = NgramDictionary(filtered_tokenized_sentences, segmentations, audio_paths, gram=3, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build German Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_root = '/mnt/data/siqiouyang/datasets/covost2/de'\n",
    "de_df = load_df_from_tsv(os.path.join(de_root, 'train_st_de_en.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_sentences = de_df['src_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_tokenized_sentences = []\n",
    "for sent in de_sentences:\n",
    "    sent = ''.join(c for c in sent if c not in punctuation)\n",
    "    tokens = [tok for tok in sent.split(' ') if tok != '']\n",
    "    de_tokenized_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id, tokens in zip(de_df['id'], tokenized_sentences):\n",
    "#     with open(os.path.join(de_root, '16kHz', '{}.txt'.format(id)), 'w') as w:\n",
    "#         w.write(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_gram = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_de_tokens = set()\n",
    "de_freq = defaultdict(int)\n",
    "for tokens in de_tokenized_sentences:\n",
    "    for idx in range(len(tokens)):\n",
    "        for g in range(min(de_gram, len(tokens) - idx)):\n",
    "            token = ' '.join(tokens[idx: idx + g + 1])\n",
    "            all_de_tokens.add(token)\n",
    "            de_freq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_token = sum(de_freq.values())\n",
    "n_total_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_embs = model.encode(list(all_de_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_embs.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = zh_dict.search(de_embs, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (D > 0.9).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_de_tokens = list(all_de_tokens)\n",
    "n_total_found = 0\n",
    "for idx in range(len(all_de_tokens)):\n",
    "    if mask[idx]:\n",
    "        n_total_found += de_freq[all_de_tokens[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_found / n_total_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.array(all_de_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = {}\n",
    "for w_de, w_zh in zip(tokens[mask], zh_dict.id2tok[I.flatten()[mask]]):\n",
    "    match[w_de] = w_zh\n",
    "    print(w_de, w_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to place a language tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Chinese Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_root = '/mnt/data/siqiouyang/datasets/covost2/zh-CN'\n",
    "zh_df = load_df_from_tsv(os.path.join(zh_root, 'train_st_zh-CN_en.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_sentences = zh_df['src_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_tokenized_sentences = []\n",
    "with open('/home/siqiouyang/work/projects/WMSeg/zh.txt.tok', 'r') as r:\n",
    "    for line in r.readlines():\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            line = punc.sub(' ', line)\n",
    "            tokens = [tok for tok in line.split(' ') if tok != '']\n",
    "            zh_tokenized_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zh_tokens = set()\n",
    "zh_freq = defaultdict(int)\n",
    "for tokens in zh_tokenized_sentences:\n",
    "    for token in tokens:\n",
    "        all_zh_tokens.add(token)\n",
    "        zh_freq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_token = sum(zh_freq.values())\n",
    "n_total_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_embs = model.encode(list(all_zh_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_embs.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = de_dict.search(zh_embs, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (D > 0.9).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zh_tokens = list(all_zh_tokens)\n",
    "n_total_found = 0\n",
    "for idx in range(len(all_zh_tokens)):\n",
    "    if mask[idx]:\n",
    "        n_total_found += zh_freq[all_zh_tokens[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_found / n_total_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.array(all_zh_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = {}\n",
    "for w_zh, w_de in zip(tokens[mask], de_dict.id2tok[I.flatten()[mask]]):\n",
    "    match[w_zh] = w_de\n",
    "    print(w_zh, w_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix Chinese Audio into German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_punctuation = punctuation + '„“”‚’«»ʿ‹›‘'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'de-zh-3g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zh_root = '/mnt/data/siqiouyang/datasets/covost2/{}'.format(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silence = th.zeros(1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pass = 0\n",
    "n_notpass = 0\n",
    "for id, transcript in zip(tqdm(de_df['id']), de_df['src_text']):\n",
    "    grid_path = os.path.join(de_zh_root, '16kHz/align/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        tokens = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        transcript = ''.join(c for c in transcript if c not in de_punctuation)\n",
    "        ori_tokens = [tok for tok in transcript.split(' ') if tok != '']\n",
    "\n",
    "        string = ' '.join([tok.text.lower() for tok in tokens])\n",
    "        ori_string = ' '.join([tok.lower() for tok in ori_tokens])\n",
    "\n",
    "        if string == ori_string:\n",
    "            audio_path = os.path.join(de_zh_root, '16kHz/{}.wav'.format(id))\n",
    "            waveform = torchaudio.load(audio_path)[0][0]\n",
    "            n_frame = waveform.size(0)\n",
    "            frame_rate = 16000\n",
    "            duration = n_frame / frame_rate\n",
    "\n",
    "            new_waveform = []\n",
    "            new_tokens = []\n",
    "            \n",
    "            intervals = (np.array([(tok.xmin, tok.xmax) for tok in tokens]) * frame_rate).astype(int)\n",
    "\n",
    "            average_volume = 0.\n",
    "            word_duration = 0.\n",
    "            for xmin, xmax in intervals:\n",
    "                word_duration += xmax - xmin\n",
    "                average_volume += waveform[xmin : xmax].abs().sum()\n",
    "            average_volume /= word_duration\n",
    "\n",
    "            mask = []\n",
    "            orig_intervals = []\n",
    "            mixed_intervals = []\n",
    "            last_end = idx = 0\n",
    "            prefix_length = 0.\n",
    "            while idx < len(intervals):\n",
    "                xmin, xmax = intervals[idx]\n",
    "                new_waveform.append(waveform[last_end : xmin])\n",
    "                prefix_length += xmin - last_end\n",
    "                replace = False\n",
    "                for g in range(min(de_gram, len(intervals) - idx), 0, -1):\n",
    "                    token = ' '.join(ori_tokens[idx : idx + g])\n",
    "                    if token in match:\n",
    "                        replace = True\n",
    "                        token_zh = match[token]\n",
    "                        selectable_waveforms = zh_dict.waveforms[zh_dict.tok2id[token_zh]]\n",
    "\n",
    "                        selected_waveform = random.choice(selectable_waveforms)\n",
    "                        selected_volume = selected_waveform.abs().mean()\n",
    "                        selected_waveform *= average_volume / selected_volume\n",
    "\n",
    "                        sil_selected_waveform = th.cat([silence, selected_waveform, silence], dim=0)\n",
    "\n",
    "                        new_waveform.append(sil_selected_waveform)\n",
    "                        new_tokens.append(token_zh)\n",
    "                        last_end = intervals[idx + g - 1][1]\n",
    "                        idx += g\n",
    "                        mask.append(True)\n",
    "                        orig_intervals.append((xmin, last_end))\n",
    "                        mixed_intervals.append((\n",
    "                            prefix_length + silence.size(0), \n",
    "                            prefix_length + silence.size(0) + selected_waveform.size(0)\n",
    "                        ))\n",
    "                        prefix_length += silence.size(0) * 2 + selected_waveform.size(0)\n",
    "                        break\n",
    "                if not replace:\n",
    "                    new_waveform.append(waveform[xmin : xmax])\n",
    "                    new_tokens.append(ori_tokens[idx])\n",
    "                    last_end = xmax\n",
    "                    idx += 1\n",
    "                    mask.append(False)\n",
    "                    orig_intervals.append((xmin, xmax))\n",
    "                    mixed_intervals.append((\n",
    "                        prefix_length,\n",
    "                        prefix_length + xmax - xmin\n",
    "                    ))\n",
    "                    prefix_length += xmax - xmin\n",
    "\n",
    "            new_waveform = th.cat(new_waveform, dim=0).unsqueeze(0)\n",
    "            new_audio_path = os.path.join(de_zh_root, '16kHz/{}-mixed.wav'.format(id))\n",
    "            torchaudio.save(new_audio_path, new_waveform, sample_rate=16000)\n",
    "\n",
    "            mask = th.tensor(mask, dtype=bool)\n",
    "            orig_intervals = th.tensor(orig_intervals, dtype=float) / n_frame\n",
    "            mixed_intervals = th.tensor(mixed_intervals, dtype=float) / new_waveform.size(1)\n",
    "            match_path = os.path.join(de_zh_root, '16kHz/{}.pt'.format(id))\n",
    "            th.save([mask, orig_intervals, mixed_intervals], match_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_matched = 0\n",
    "for id, transcript in zip(tqdm(de_df['id']), de_df['src_text']):\n",
    "    grid_path = os.path.join(de_zh_root, '16kHz/align/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        tokens = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        transcript = ''.join(c for c in transcript if c not in de_punctuation)\n",
    "        ori_tokens = [tok for tok in transcript.split(' ') if tok != '']\n",
    "\n",
    "        string = ' '.join([tok.text.lower() for tok in tokens])\n",
    "        ori_string = ' '.join([tok.lower() for tok in ori_tokens])\n",
    "\n",
    "        if string == ori_string:\n",
    "            for token in ori_tokens:\n",
    "                replace_flag = np.random.rand() > 0.5\n",
    "                if token in match and replace_flag:\n",
    "                    n_total_matched += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_matched / n_total_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zh_df1 = load_df_from_tsv(os.path.join(de_zh_root, 'train_st_de-zh-3g_en.tsv'))\n",
    "de_zh_df2 = load_df_from_tsv(os.path.join(de_zh_root, 'train_st_de-zh-3g_en_phone.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain = []\n",
    "for id in de_zh_df1['id']:\n",
    "    if os.path.exists(os.path.join(de_zh_root, '16kHz/{}-mixed.wav'.format(id))):\n",
    "        retain.append(True)\n",
    "    else:\n",
    "        retain.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zh_df1_filtered = de_zh_df1.loc[retain]\n",
    "de_zh_df2_filtered = de_zh_df2.loc[retain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zh_df1_filtered['src_lang'] = [version] * len(de_zh_df1_filtered)\n",
    "de_zh_df2_filtered['src_lang'] = [version] * len(de_zh_df2_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_to_tsv(de_zh_df1_filtered, os.path.join(de_zh_root, 'train_st_{}_en.tsv'.format(version)))\n",
    "save_df_to_tsv(de_zh_df2_filtered, os.path.join(de_zh_root, 'train_st_{}_en_phone.tsv'.format(version)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix German Audio into Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_de_root = '/mnt/data/siqiouyang/datasets/covost2/zh-de-3g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pass = 0\n",
    "n_notpass = 0\n",
    "for id, ori_tokens in zip(tqdm(zh_df['id']), zh_tokenized_sentences):\n",
    "    grid_path = os.path.join(zh_de_root, '16kHz/align_wmseg/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        tokens = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        assert len(tokens) == len(ori_tokens)\n",
    "\n",
    "        audio_path = os.path.join(zh_de_root, '16kHz/{}.wav'.format(id))\n",
    "        waveform = torchaudio.load(audio_path)[0][0]\n",
    "        n_frame = waveform.size()\n",
    "        frame_rate = 16000\n",
    "\n",
    "        new_waveform = []\n",
    "        \n",
    "        intervals = (np.array([(tok.xmin, tok.xmax) for tok in tokens]) * frame_rate).astype(int)\n",
    "        last_end = 0\n",
    "        for token, (xmin, xmax) in zip(ori_tokens, intervals):\n",
    "            new_waveform.append(waveform[last_end : xmin])\n",
    "\n",
    "            if token in match:\n",
    "                token_de = match[token]\n",
    "                selectable_waveforms = de_dict.waveforms[de_dict.tok2id[token_de]]\n",
    "                new_waveform.append(random.choice(selectable_waveforms))\n",
    "            else:\n",
    "                new_waveform.append(waveform[xmin : xmax])\n",
    "\n",
    "            last_end = xmax\n",
    "\n",
    "        new_waveform = th.cat(new_waveform, dim=0).unsqueeze(0)\n",
    "        torchaudio.save(audio_path, new_waveform, sample_rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust Number of Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'de-zh-3g-hf'\n",
    "adj_root = '/mnt/data/siqiouyang/datasets/covost2/' + version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_df1 = load_df_from_tsv(os.path.join(adj_root, 'train_st_{}_en.tsv'.format(version)))\n",
    "adj_df2 = load_df_from_tsv(os.path.join(adj_root, 'train_st_{}_en_phone.tsv'.format(version)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adj_df in [adj_df1, adj_df2]:\n",
    "    n_frames = []\n",
    "    for id in tqdm(adj_df['id']):\n",
    "        info = torchaudio.info(os.path.join(adj_root, '16kHz/{}.wav'.format(id)))\n",
    "        n_frames.append(info.num_frames)\n",
    "    adj_df['n_frames'] = n_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_to_tsv(adj_df1, os.path.join(adj_root, 'train_st_{}_en.tsv'.format(version)))\n",
    "save_df_to_tsv(adj_df2, os.path.join(adj_root, 'train_st_{}_en_phone.tsv'.format(version)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_punctuation = punctuation + '„“”‚’«»ʿ‹›‘'\n",
    "de_zh_root = '/mnt/data/siqiouyang/datasets/covost2/de'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(de_df)))\n",
    "random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silence = th.zeros(1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_replaced = 0\n",
    "n_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_origs = []\n",
    "de_mixeds = []\n",
    "de_audios = []\n",
    "de_orig_audios = []\n",
    "for idx in tqdm(indices[:n_max]):\n",
    "    id = de_df['id'][idx]\n",
    "    transcript = de_df['src_text'][idx]\n",
    "    grid_path = os.path.join(de_zh_root, '16kHz/align/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        tokens = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        transcript = ''.join(c for c in transcript if c not in de_punctuation)\n",
    "        ori_tokens = [tok for tok in transcript.split(' ') if tok != '']\n",
    "\n",
    "        string = ' '.join([tok.text.lower() for tok in tokens])\n",
    "        ori_string = ' '.join([tok.lower() for tok in ori_tokens])\n",
    "\n",
    "        if string == ori_string:\n",
    "            audio_path = os.path.join(de_zh_root, '16kHz/{}.wav'.format(id))\n",
    "            waveform = torchaudio.load(audio_path)[0][0]\n",
    "            n_frame = waveform.size()\n",
    "            frame_rate = 16000\n",
    "\n",
    "            new_waveform = []\n",
    "            new_tokens = []\n",
    "            \n",
    "            intervals = (np.array([(tok.xmin, tok.xmax) for tok in tokens]) * frame_rate).astype(int)\n",
    "\n",
    "            average_volume = 0.\n",
    "            total_duration = 0.\n",
    "            for xmin, xmax in intervals:\n",
    "                total_duration += xmax - xmin\n",
    "                average_volume += waveform[xmin : xmax].abs().sum()\n",
    "            average_volume /= total_duration\n",
    "\n",
    "            last_end = 0\n",
    "\n",
    "            idx = 0\n",
    "            while idx < len(intervals):\n",
    "                xmin, xmax = intervals[idx]\n",
    "                new_waveform.append(waveform[last_end : xmin])\n",
    "                replace = False\n",
    "                for g in range(min(de_gram, len(intervals) - idx), 0, -1):\n",
    "                    token = ' '.join(ori_tokens[idx : idx + g])\n",
    "                    if token in match:\n",
    "                        replace = True\n",
    "                        token_zh = match[token]\n",
    "                        selectable_waveforms = zh_dict.waveforms[zh_dict.tok2id[token_zh]]\n",
    "\n",
    "                        selected_waveform = random.choice(selectable_waveforms)\n",
    "                        selected_volume = selected_waveform.abs().mean()\n",
    "                        selected_waveform *= average_volume / selected_volume\n",
    "\n",
    "                        selected_waveform = th.cat([silence, selected_waveform, silence], dim=0)\n",
    "\n",
    "                        new_waveform.append(selected_waveform)\n",
    "                        new_tokens.append(token_zh)\n",
    "                        last_end = intervals[idx + g - 1][1]\n",
    "                        idx += g\n",
    "                        n_replaced += g\n",
    "                        n_total += g\n",
    "                        break\n",
    "                if not replace:\n",
    "                    new_waveform.append(waveform[xmin : xmax])\n",
    "                    new_tokens.append(ori_tokens[idx])\n",
    "                    last_end = xmax\n",
    "                    idx += 1\n",
    "                    n_total += 1\n",
    "\n",
    "            # for token, (xmin, xmax) in zip(ori_tokens, intervals):\n",
    "            #     new_waveform.append(waveform[last_end : xmin])\n",
    "\n",
    "            #     replace_flag = True # np.random.rand() > 0.5\n",
    "\n",
    "            #     if token in match and replace_flag:\n",
    "            #         token_zh = match[token]\n",
    "            #         selectable_waveforms = zh_dict.waveforms[zh_dict.tok2id[token_zh]]\n",
    "            #         new_waveform.append(random.choice(selectable_waveforms))\n",
    "            #         new_tokens.append(match[token])\n",
    "            #     else:\n",
    "            #         new_waveform.append(waveform[xmin : xmax])\n",
    "            #         new_tokens.append(token)\n",
    "\n",
    "            #     last_end = xmax\n",
    "\n",
    "            new_waveform = th.cat(new_waveform, dim=0).unsqueeze(0)\n",
    "            new_audio_path = 'de-zh-sample/{}.wav'.format(len(de_origs))\n",
    "            orig_audio_path = 'de-zh-sample/{}-orig.wav'.format(len(de_origs))\n",
    "            torchaudio.save(new_audio_path, new_waveform, sample_rate=16000)\n",
    "            torchaudio.save(orig_audio_path, waveform.unsqueeze(0), sample_rate=16000)\n",
    "\n",
    "            de_origs.append(' '.join(ori_tokens))\n",
    "            de_mixeds.append(' '.join(new_tokens))\n",
    "            de_audios.append(new_audio_path)\n",
    "            de_orig_audios.append(orig_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_replaced / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '<table>\\n'\n",
    "string += '\\t<tr>\\n\\t\\t<th>de-orig</th>\\n\\t\\t<th>audio-orig</th>\\n\\t\\t<th>de-mixed</th>\\n\\t\\t<th>audio-mixed</th>\\n\\t</tr>\\n'\n",
    "for de_orig, de_mixed, de_audio, de_orig_audio in zip(de_origs, de_mixeds, de_audios, de_orig_audios):\n",
    "    string += '\\t<tr>\\n\\t\\t<td>{}</td>\\n\\t\\t<td>{}</td>\\n\\t\\t<td>{}</td>\\n\\t\\t<td>{}</td>\\n\\t</tr>\\n'.format(\n",
    "        de_orig, \n",
    "        '<audio controls><source src=\"{}\" type=\"audio/wav\"></audio>'.format(de_orig_audio),\n",
    "        de_mixed,\n",
    "        '<audio controls><source src=\"{}\" type=\"audio/wav\"></audio>'.format(de_audio)\n",
    "    )\n",
    "string += '</table>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('de-zh-sample.html', 'w', encoding='utf-8') as w:\n",
    "    w.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_origs = []\n",
    "de_orig_intervals = []\n",
    "de_mixeds = []\n",
    "de_mixed_intervals = []\n",
    "de_audios = []\n",
    "de_orig_audios = []\n",
    "for idx in tqdm(indices[:n_max]):\n",
    "    id = de_df['id'][idx]\n",
    "    transcript = de_df['src_text'][idx]\n",
    "    grid_path = os.path.join(de_zh_root, '16kHz/align/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        tokens = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        transcript = ''.join(c for c in transcript if c not in de_punctuation)\n",
    "        ori_tokens = [tok for tok in transcript.split(' ') if tok != '']\n",
    "\n",
    "        string = ' '.join([tok.text.lower() for tok in tokens])\n",
    "        ori_string = ' '.join([tok.lower() for tok in ori_tokens])\n",
    "\n",
    "        if string == ori_string:\n",
    "            audio_path = os.path.join(de_zh_root, '16kHz/{}.wav'.format(id))\n",
    "            waveform = torchaudio.load(audio_path)[0][0]\n",
    "            n_frame = waveform.size()\n",
    "            frame_rate = 16000\n",
    "\n",
    "            new_waveform = []\n",
    "            new_tokens = []\n",
    "\n",
    "            orig_intervals = []\n",
    "            new_intervals = []\n",
    "            \n",
    "            intervals = (np.array([(tok.xmin, tok.xmax) for tok in tokens]) * frame_rate).astype(int)\n",
    "            last_end = 0\n",
    "            sum_lengths = 0\n",
    "            for token, (xmin, xmax) in zip(ori_tokens, intervals):\n",
    "                new_waveform.append(waveform[last_end : xmin])\n",
    "                sum_lengths += xmin - last_end\n",
    "\n",
    "                replace_flag = True # np.random.rand() > 0.5\n",
    "\n",
    "                orig_intervals.append((xmin, xmax))\n",
    "\n",
    "                if token in match and replace_flag:\n",
    "                    token_zh = match[token]\n",
    "                    selectable_waveforms = zh_dict.waveforms[zh_dict.tok2id[token_zh]]\n",
    "                    selected_waveform = random.choice(selectable_waveforms)\n",
    "                    new_waveform.append(selected_waveform)\n",
    "                    new_tokens.append(match[token])\n",
    "                    new_intervals.append((sum_lengths, sum_lengths + selected_waveform.size(0)))\n",
    "                    sum_lengths += selected_waveform.size(0)\n",
    "                else:\n",
    "                    new_waveform.append(waveform[xmin : xmax])\n",
    "                    new_tokens.append(token)\n",
    "                    new_intervals.append((sum_lengths, sum_lengths + xmax - xmin))\n",
    "                    sum_lengths += xmax - xmin\n",
    "\n",
    "                last_end = xmax\n",
    "\n",
    "            new_waveform = th.cat(new_waveform, dim=0).unsqueeze(0)\n",
    "            new_audio_path = 'de-zh-sample/{}.wav'.format(len(de_origs))\n",
    "            orig_audio_path = 'de-zh-sample/{}-orig.wav'.format(len(de_origs))\n",
    "            torchaudio.save(new_audio_path, new_waveform, sample_rate=16000)\n",
    "            torchaudio.save(orig_audio_path, waveform.unsqueeze(0), sample_rate=16000)\n",
    "\n",
    "            de_origs.append(ori_tokens)\n",
    "            de_orig_intervals.append(orig_intervals)\n",
    "            de_mixeds.append(new_tokens)\n",
    "            de_mixed_intervals.append(new_intervals)\n",
    "            de_audios.append(new_audio_path)\n",
    "            de_orig_audios.append(orig_audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "task = Namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(vocab_filename):\n",
    "    _dict_path = vocab_filename\n",
    "    if not os.path.isfile(_dict_path):\n",
    "        raise FileNotFoundError(f\"Dict not found: {_dict_path}\")\n",
    "    _dict = Dictionary.load(_dict_path)\n",
    "    for code in codes:\n",
    "        _dict.add_symbol(MultilingualTripletDataset.LANG_TAG_TEMPLATE.format(code))\n",
    "    _dict.add_symbol('<mask>')\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_list_filename = '/mnt/data/siqiouyang/runs/mST/pretrained/mbart50.ft.n1/ML50_langs.txt'\n",
    "vocab_filename = '/mnt/data/siqiouyang/runs/mST/pretrained/mbart50.ft.n1/dict.txt'\n",
    "phone_vocab_filename = '/mnt/data/siqiouyang/datasets/covost2/phone_dict.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = MultilingualTripletDataset.get_lang_codes(lang_list_filename)\n",
    "dict = load_dict(vocab_filename)\n",
    "with open(phone_vocab_filename, 'r') as r:\n",
    "    phone_list = [l.strip() for l in r.readlines() if l.strip() != '']\n",
    "    phone_dict = {l: idx + 1 for idx, l in enumerate(phone_list)} # leave 0 as blank\n",
    "    phone_list = ['|'] + phone_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.src_dict = task.tgt_dict = dict\n",
    "task.phone_dict = phone_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.w2v2_model_path = '/mnt/data/siqiouyang/runs/mST/pretrained/xlsr2_300m.pt'\n",
    "args.mbart50_dir = '/mnt/data/siqiouyang/runs/mST/pretrained/mbart50.ft.n1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = W2V2Transformer.build_model(args, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/mnt/data/siqiouyang/runs/mST/xlsr_phone_mbart_zh_de-zh/checkpoint_13_10000.pt'\n",
    "ckpt = load_checkpoint_to_cpu(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict.symbols[-55:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(path, lang_tag):\n",
    "    source = get_features_or_waveform(\n",
    "        path,\n",
    "        need_waveform=True,\n",
    "        sample_rate=16000,\n",
    "    )\n",
    "    source = th.from_numpy(source).float()\n",
    "    src_tokens = source.unsqueeze(0).to(device)\n",
    "    src_lengths = th.LongTensor([source.size(0)]).to(device)\n",
    "    src_lang_tag_idx = dict.index(lang_tag)\n",
    "    src_lang_tag_indices = th.LongTensor([src_lang_tag_idx]).unsqueeze(-1).to(device)\n",
    "    encoder_out = model.encoder(src_tokens, src_lengths, src_lang_tag_indices)\n",
    "    return encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'de-zh-sample/1-orig.wav'\n",
    "info = torchaudio.info(path)\n",
    "orig_encoder_out = compute(path, '<lang:de_DE>')\n",
    "orig_length, _, d = orig_encoder_out.encoder_out.size()\n",
    "orig_interval = ((np.array(de_orig_intervals[1]) / info.num_frames) * orig_length).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'de-zh-sample/1.wav'\n",
    "info = torchaudio.info(path)\n",
    "mixed_encoder_out = compute(path, '<lang:de_DE>')\n",
    "mixed_length, _, d = mixed_encoder_out.encoder_out.size()\n",
    "mixed_interval = ((np.array(de_orig_intervals[1]) / info.num_frames) * mixed_length).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = orig_encoder_out.encoder_out[47:56].mean(dim=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = mixed_encoder_out.encoder_out[56:65].mean(dim=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = mixed_encoder_out.encoder_out[47:55].mean(dim=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A * B).sum() / A.norm() / B.norm(), ((A - B) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A * C).sum() / A.norm() / C.norm(), ((A - C) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27e528650793650579273e035796a30e6ea4eb9cce70b5c065b2912b46e06367"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('st')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
