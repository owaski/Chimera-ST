{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from argparse import Namespace\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "\n",
    "import IPython\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sentencepiece as spm\n",
    "\n",
    "import textgrids\n",
    "import faiss\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from fairseq.data import Dictionary\n",
    "from fairseq.models.mST.w2v2_phone_transformer import W2V2Transformer\n",
    "from fairseq.data.audio.multilingual_triplet_v2_phone_dataset import (\n",
    "    MultilingualTripletDataConfig,\n",
    "    MultilingualTripletDataset,\n",
    "    MultilingualTripletDatasetCreator\n",
    ")\n",
    "from fairseq.data.audio.speech_to_text_dataset import get_features_or_waveform\n",
    "from examples.speech_to_text.data_utils import load_df_from_tsv, save_df_to_tsv\n",
    "from fairseq.checkpoint_utils import load_checkpoint_to_cpu\n",
    "from fairseq.data.encoders.sentencepiece_bpe import SentencepieceBPE, SentencepieceConfig\n",
    "\n",
    "from fairseq.models.mST.w2v2_phone_transformer import W2V2Transformer\n",
    "from fairseq.data.audio.multilingual_triplet_v2_phone_dataset import (\n",
    "    MultilingualTripletDataConfig,\n",
    "    MultilingualTripletDataset,\n",
    "    MultilingualTripletDatasetCreator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process CV-9.0 Zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_zh_root = '/mnt/data/siqiouyang/datasets/cv-corpus-9.0-2022-04-27/zh-CN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_zh_df = load_df_from_tsv(os.path.join(cv_zh_root, 'validated.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accents</th>\n",
       "      <th>locale</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02ec74191c6ccc7dcf6ecaa217268263c477273b4de93f...</td>\n",
       "      <td>common_voice_zh-CN_22069600.mp3</td>\n",
       "      <td>宋朝末年年间定居粉岭围。</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>zh-CN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0431cf00d4491b99a93700d7aa0b1948a057b2c162a620...</td>\n",
       "      <td>common_voice_zh-CN_22006851.mp3</td>\n",
       "      <td>渐渐行动不便</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>zh-CN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04742f27bccab99619bd4ec3f256b36c639afd058c8664...</td>\n",
       "      <td>common_voice_zh-CN_22115132.mp3</td>\n",
       "      <td>二十一年去世。</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>zh-CN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0648def3862cbb968eec23fad967f50e35fc8e0eea67b4...</td>\n",
       "      <td>common_voice_zh-CN_22120171.mp3</td>\n",
       "      <td>他们自称恰哈拉。</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>zh-CN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0697ece1f99a08477906d0f3b4e74e1d6ffca76c20a7db...</td>\n",
       "      <td>common_voice_zh-CN_18646658.mp3</td>\n",
       "      <td>局部干涩的例子包括有口干、眼睛干燥、及阴道干燥。</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>zh-CN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48511</th>\n",
       "      <td>25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...</td>\n",
       "      <td>common_voice_zh-CN_19717327.mp3</td>\n",
       "      <td>这被共和派和社会主义者称为一次巨大胜利。</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>thirties</td>\n",
       "      <td>female</td>\n",
       "      <td>出生地：31 上海市</td>\n",
       "      <td>zh-CN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48512</th>\n",
       "      <td>25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...</td>\n",
       "      <td>common_voice_zh-CN_19717330.mp3</td>\n",
       "      <td>汉默史密斯是伦敦的一大波兰人聚居地。</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>thirties</td>\n",
       "      <td>female</td>\n",
       "      <td>出生地：31 上海市</td>\n",
       "      <td>zh-CN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48513</th>\n",
       "      <td>25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...</td>\n",
       "      <td>common_voice_zh-CN_19717333.mp3</td>\n",
       "      <td>被处理的晶片试样放置于真空室中的样品架上。</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>thirties</td>\n",
       "      <td>female</td>\n",
       "      <td>出生地：31 上海市</td>\n",
       "      <td>zh-CN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48514</th>\n",
       "      <td>25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...</td>\n",
       "      <td>common_voice_zh-CN_19717335.mp3</td>\n",
       "      <td>曾连获三届国家新闻出版总署颁发的国家期刊奖。</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>thirties</td>\n",
       "      <td>female</td>\n",
       "      <td>出生地：31 上海市</td>\n",
       "      <td>zh-CN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48515</th>\n",
       "      <td>25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...</td>\n",
       "      <td>common_voice_zh-CN_19717367.mp3</td>\n",
       "      <td>古希腊作家之一。</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>thirties</td>\n",
       "      <td>female</td>\n",
       "      <td>出生地：31 上海市</td>\n",
       "      <td>zh-CN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48516 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               client_id  \\\n",
       "0      02ec74191c6ccc7dcf6ecaa217268263c477273b4de93f...   \n",
       "1      0431cf00d4491b99a93700d7aa0b1948a057b2c162a620...   \n",
       "2      04742f27bccab99619bd4ec3f256b36c639afd058c8664...   \n",
       "3      0648def3862cbb968eec23fad967f50e35fc8e0eea67b4...   \n",
       "4      0697ece1f99a08477906d0f3b4e74e1d6ffca76c20a7db...   \n",
       "...                                                  ...   \n",
       "48511  25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...   \n",
       "48512  25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...   \n",
       "48513  25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...   \n",
       "48514  25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...   \n",
       "48515  25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...   \n",
       "\n",
       "                                  path                  sentence  up_votes  \\\n",
       "0      common_voice_zh-CN_22069600.mp3              宋朝末年年间定居粉岭围。         2   \n",
       "1      common_voice_zh-CN_22006851.mp3                    渐渐行动不便         2   \n",
       "2      common_voice_zh-CN_22115132.mp3                   二十一年去世。         2   \n",
       "3      common_voice_zh-CN_22120171.mp3                  他们自称恰哈拉。         2   \n",
       "4      common_voice_zh-CN_18646658.mp3  局部干涩的例子包括有口干、眼睛干燥、及阴道干燥。         2   \n",
       "...                                ...                       ...       ...   \n",
       "48511  common_voice_zh-CN_19717327.mp3      这被共和派和社会主义者称为一次巨大胜利。         2   \n",
       "48512  common_voice_zh-CN_19717330.mp3        汉默史密斯是伦敦的一大波兰人聚居地。         2   \n",
       "48513  common_voice_zh-CN_19717333.mp3     被处理的晶片试样放置于真空室中的样品架上。         2   \n",
       "48514  common_voice_zh-CN_19717335.mp3    曾连获三届国家新闻出版总署颁发的国家期刊奖。         2   \n",
       "48515  common_voice_zh-CN_19717367.mp3                  古希腊作家之一。         2   \n",
       "\n",
       "       down_votes       age  gender     accents locale segment  \n",
       "0               0                                zh-CN          \n",
       "1               0                                zh-CN          \n",
       "2               0                                zh-CN          \n",
       "3               0                                zh-CN          \n",
       "4               1                                zh-CN          \n",
       "...           ...       ...     ...         ...    ...     ...  \n",
       "48511           0  thirties  female  出生地：31 上海市  zh-CN          \n",
       "48512           1  thirties  female  出生地：31 上海市  zh-CN          \n",
       "48513           0  thirties  female  出生地：31 上海市  zh-CN          \n",
       "48514           1  thirties  female  出生地：31 上海市  zh-CN          \n",
       "48515           0  thirties  female  出生地：31 上海市  zh-CN          \n",
       "\n",
       "[48516 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_zh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in tqdm(cv_zh_df['path']):\n",
    "#     audio_path = os.path.join(cv_zh_root, 'clips/{}'.format(path))\n",
    "#     waveform, sample_rate = torchaudio.load(audio_path)\n",
    "#     resampled_waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
    "#     torchaudio.save(os.path.join(cv_zh_root, '16kHz/{}.wav'.format(path[:-4])), resampled_waveform, sample_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['宋朝末年年间定居粉岭围。',\n",
       " '渐渐行动不便',\n",
       " '二十一年去世。',\n",
       " '他们自称恰哈拉。',\n",
       " '局部干涩的例子包括有口干、眼睛干燥、及阴道干燥。',\n",
       " '嘉靖三十八年，登进士第三甲第二名。',\n",
       " '这一名称一直沿用至今。',\n",
       " '阿列河畔贝赛。',\n",
       " '同时乔凡尼还得到包税合同和许多明矾矿的经营权。',\n",
       " '为了惩罚西扎城和塞尔柱的结盟，盟军在抵达后将外城烧毁。',\n",
       " '河内盛产黄色无鱼鳞的鳍射鱼。',\n",
       " '毛柄新木姜子为樟科新木姜子属下的一个变种。',\n",
       " '他主要演出泰米尔语电影。',\n",
       " '粗体字表示为主演。',\n",
       " '福崎町是位于日本兵库县中部的行政区划。',\n",
       " '下行月台设有厕所。',\n",
       " '耶尔河畔圣伊莱尔人口变化图示',\n",
       " '光绪八年再中举人。',\n",
       " '台湾北部地区家庭多以在农历年前时段包润饼则是在清明期间。',\n",
       " '蔡声白。',\n",
       " '该区舰队主要负责为公海舰队的战列分舰队提供屏护。',\n",
       " '雷诺在回归的第一年比赛中以第四名的成绩完成了比赛。',\n",
       " '这样都可以啊',\n",
       " '此原理也广泛应用于家庭之中用于生产软水。',\n",
       " '本片的导演是赵秀贤和梁铉锡。',\n",
       " '大戟亚科是大戟科旗下三个亚科的其中之一。',\n",
       " '奥特拉德诺耶农村居民点是俄罗斯联邦沃罗涅日州新乌斯曼区所属的一个农村居民点。',\n",
       " '吉内斯塔。',\n",
       " '其名称来源于当时图书馆的创始人。',\n",
       " '长期信用银行法。',\n",
       " '主要用户是法国陆军以及阿拉伯联合大公国。',\n",
       " '如今该符号已变成一个表示迷因的网络词汇。',\n",
       " '戴七宝冠作通身光。',\n",
       " '最少数百名购不到门票的影迷要求加场。',\n",
       " '她有两个哥哥。',\n",
       " '七',\n",
       " '尽力和武市半平太一起创立土佐勤王党。',\n",
       " '他也是蚬壳电器创办人翁佑的女婿。',\n",
       " '厚实的墙壁让旧机内部冬暖夏凉。',\n",
       " '槽果扁莎为莎草科扁莎属下的一个种。',\n",
       " '正模标本是多个可能的生物型中的一个。',\n",
       " '乌大经清朝军事人物。',\n",
       " '卡尔雷尔。',\n",
       " '滨江县先后隶属于吉林省西北路道和滨江道。',\n",
       " '大使馆全年都为各类政治人物和公众举办了许多活动，包括庆祝以色列独立日独立日。',\n",
       " '苏光彩之子。',\n",
       " '感冒茶起源于中国岭南一带。',\n",
       " '拉沙尔克。',\n",
       " '伍斯特座堂是英格兰伍斯特郡的一座教堂。',\n",
       " '电子浩室是浩室音乐的一个类型。',\n",
       " '韦拉是巴西马托格罗索州的一个市镇。',\n",
       " '直到小的支气管内均有软骨存在。',\n",
       " '本站主体结构经已完工封顶。',\n",
       " '光果婆婆纳为车前草科婆婆纳属下的一个种。',\n",
       " '正要去房间小床睡觉',\n",
       " '但其他价含氧酸不遵循此规律。',\n",
       " '崔明逃到鲁国。',\n",
       " '纳塔兰迪亚是巴西米纳斯吉拉斯州的一个市镇。',\n",
       " '充满哲学元素的实验性作品。',\n",
       " '各级地方政府已经大幅减少健康步道的铺设。',\n",
       " '福茨斯普林斯是位于美国加利福尼亚州科卢萨县的一个非建制地区。',\n",
       " '当她去世时，讣告也称她为「永不沉没的布朗夫人」。',\n",
       " '马尔萨克。',\n",
       " '二人育有两子。',\n",
       " '业务在全球经营食品的加工及贸易业务。',\n",
       " '有论者批评词中出现现代通俗口语「劲歌」，与整首歌的古典风格格格不入。',\n",
       " '曲阜师范大学是山东省以师范教育为主的省属重点大学。',\n",
       " '然而此张单曲却成为转捩点。',\n",
       " '城站站主体位于位于城站路与西湖大道交叉路口东侧站前广场地下。',\n",
       " '南木藓为塔藓科南木藓属下的一个种。',\n",
       " '二',\n",
       " '中野富士见町站的铁路车站。',\n",
       " '正巧母亲往外探头',\n",
       " '约旦河谷的东岸即为约旦。',\n",
       " '沙滩巧固球运动目前是台湾巧固球协会推展的主要方向之一',\n",
       " '胶北街道是中国山东省青岛市胶州市下辖的一个街道。',\n",
       " '县治申特维尔。',\n",
       " '今村明恒毕业于东京帝国大学并留校任职。',\n",
       " '在他们的登陆地附近发展起来的那座镇子现在也被叫作了马德林港。',\n",
       " '造成的原因是由于上学的压力和就业水平的降低。',\n",
       " '后来崛起的野草商团加入后形成现在的格局。',\n",
       " '五座大桥统称舟山跨海大桥。',\n",
       " '这个结论是在对地基土层成份进行观测后得出的。',\n",
       " '前翅近端位有一些橙点。',\n",
       " '常用的吸附物质是活性炭和树脂。',\n",
       " '多数团地内有管理事务所提供服务。',\n",
       " '大田县的历史可追溯到明朝。',\n",
       " '他的教父母为瑞典国王弗雷德里克一世。',\n",
       " '没有再回来',\n",
       " '国君为归姓。',\n",
       " '这句话后来成为了网友常提及的流行语。',\n",
       " '布里夫拉盖亚尔德机场位于其境内。',\n",
       " '毕业后从事外国影片的翻译工作。',\n",
       " '零',\n",
       " '青仓站播但线的铁路车站。',\n",
       " '昂布朗的圣昂德雷人口变化图示',\n",
       " '绑住影响血液循环',\n",
       " '月台与付费区内大厅之间设有电扶梯与电梯。',\n",
       " '与漫画家十乐寺为姐妹。',\n",
       " '原来直播是假的',\n",
       " '他妻子是北京大学分子医学研究所所长肖瑞平教授。',\n",
       " '这座大厦以苏丹马末·依斯干达命名，是的一部分。',\n",
       " '他们现时在国家橄榄球联合会北区参与国家美式足球联盟比赛。',\n",
       " '江西乡试第二十八名。',\n",
       " '周至刺绣是秦绣的代表。',\n",
       " '最令人瞩目的政绩是与美国总统杜鲁门达成了第四点计划。',\n",
       " '七',\n",
       " '他们可以做早餐',\n",
       " '中华白海豚是香港的吉祥物之一。',\n",
       " '三',\n",
       " '曾多次接受卫生福利部疾病管制署邀请拍摄安全性行为宣导海报及宣导影片。',\n",
       " '上井草是东京都杉并区的地名。',\n",
       " '后入读纽约音乐学院戏剧艺术研究表演。',\n",
       " '此事成为著名的诺贝尔奖争议之一。',\n",
       " '日姓氏族只分布在南群各社的南赛夏人。',\n",
       " '六',\n",
       " '他对镇上的新事物感到兴奋。',\n",
       " '殿试登进士第二甲第四名。',\n",
       " '他主要的工作是研究冷凝气体。',\n",
       " '属米兰总教区。',\n",
       " '张九皋。',\n",
       " '小酒吧老板天民家中休息。',\n",
       " '联邦快递球场是为了替换华盛顿红皮队之前的主场而建造的。',\n",
       " '三',\n",
       " '给谁我没有意见啊',\n",
       " '民国三年。',\n",
       " '如果你使用了它通常就很容易了。',\n",
       " '才不到九颗气球',\n",
       " '中国人民解放军少将军衔。',\n",
       " '没有表演的尾牙',\n",
       " '父亲是织田信定。',\n",
       " '北江铁线蕨为铁线蕨科铁线蕨属下的一个种。',\n",
       " '曾任北平市教育会常务理事。',\n",
       " '脱皮后一般都呈银色至深灰色渐变成黄褐色。',\n",
       " '塞尔尼松。',\n",
       " '今日的现代创作',\n",
       " '而在周六则会重播周一至五播出的节目。',\n",
       " '首都龙是华盛顿哥伦比亚特区的州恐龙。',\n",
       " '种子有束毛或有翅。',\n",
       " '弘治十五年壬戌科进士。',\n",
       " '时任中华民国总统陈水扁亦颁发褒扬令。',\n",
       " '工程在宋神宗元丰二年完成。',\n",
       " '段崇，字礼高，东汉汉中郡南郑县人。',\n",
       " '然而标题更改造成一些以预录收看的观众因未察觉变更导致第二话以后预录失败。',\n",
       " '奥昔卡因是一种有效的局部麻醉剂。',\n",
       " '亚铬酸钴在一些文献中会被误称作铬酸钴。',\n",
       " '按果实外苞片的构造可分为有刺种和无刺种两个类型。',\n",
       " '红口蝮亦是唯一一种拥有大片冠鳞及平滑背鳞的亚洲蝮蛇。',\n",
       " '行伍出身。',\n",
       " '这也是德国唯一一次未参加欧洲歌唱大赛。',\n",
       " '弗里德里希斯瓦尔德是德国勃兰登堡州的一个市镇。',\n",
       " '他籍贯中国福建莆田江口镇。',\n",
       " '锡马科沃农村居民点是俄罗斯联邦伊万诺沃州上兰杰赫区所属的一个农村居民点。',\n",
       " '希腊数学家阿尔库塔斯曾经描述过螺丝的原理。',\n",
       " '五',\n",
       " '曾任河南军区南阳军分区司令员。',\n",
       " '埃尔科里。',\n",
       " '顾安生董事长。',\n",
       " '其中大多数提到的是真正的人类的语言。',\n",
       " '李曰垓有四子一女。',\n",
       " '在法律上的不公正与尽管有瑕疵但仍有效的法之间划出一条明确的界线是不可能的。',\n",
       " '透过在地社团取信被害人',\n",
       " '隆庆五年，登进士第二甲第二十六名。',\n",
       " '曾祖父刘子定。',\n",
       " '微胖身躯散散步',\n",
       " '莫卧儿帝国创始人巴布尔大帝之子。',\n",
       " '九',\n",
       " '作者不详。',\n",
       " '幕后花絮《女神同行》于台视主频及爱奇艺播出。',\n",
       " '教育电视节目最初由教育司署独力制作。',\n",
       " '公园大道站是加拿大魁北克省蒙特利尔的一个历史性铁路车站建筑。',\n",
       " '埃尔维莱沙泰。',\n",
       " '我们必须采取措施消除它。',\n",
       " '司马弈官至黄门郎。',\n",
       " '叉斑长纺蛛为长纺蛛科长纺蛛属的动物。',\n",
       " '合萌为豆科合萌属下的一个种。',\n",
       " '印度加尔各答圣保罗座堂。',\n",
       " '七',\n",
       " '古本从少年时期当儿童演员开始投入演艺活动。',\n",
       " '三',\n",
       " '清朝乾隆四十三年。',\n",
       " '三',\n",
       " '斯莱德艺术学校在二战期间曾搬到牛津与拉斯金共同办学。',\n",
       " '亚美利加队与另一支劲旅瓜达拉哈拉芝华士的对战被视为墨西哥最重要的德比大战。',\n",
       " '被选入人教版小学教科书。',\n",
       " '表示主角在游戏中剩余的天数。',\n",
       " '籍贯广东潮阳县。',\n",
       " '网络抓取用于自动化获取万维网上的信息',\n",
       " '官至南京刑部员外郎。',\n",
       " '祥郡公阮福溪。',\n",
       " '本列表列出印度的活火山与死火山。',\n",
       " '西本正的摄影工作而闻名。',\n",
       " '淮东路大部分州县又重归宋朝。',\n",
       " '埃尔特尔莱因是德国萨克森州的一个市镇。',\n",
       " '在哲学上还有其他意义。',\n",
       " '文化大革命期间被逮捕关入秦城监狱。',\n",
       " '名称由来为唱名的前三个音调。',\n",
       " '身上总像背着什么',\n",
       " '幼虫寄主为豆科蝶形花亚科植物。',\n",
       " '讲述了主人公琉星追逐大盗九月天的过程发生的离奇故事。',\n",
       " '克莱夫蒙。',\n",
       " '担任辽宁万华企业集团有限公司董事长。',\n",
       " '社长没订到甜点却没告知',\n",
       " '三',\n",
       " '多纳维尔。',\n",
       " '这支直接向奥斯曼苏丹负责的精英部队可以分为骑兵和步兵两大集团。',\n",
       " '康熙帝派黑龙江将军萨布素前去围攻。',\n",
       " '一年后他死于霍乱。',\n",
       " '浙江碘泡虫为碘泡科碘泡虫属的动物。',\n",
       " '是',\n",
       " '里内包括下巴陵巴陵部落。',\n",
       " '但是伊泽特贝戈维奇本人一直拒绝承认这种指控。',\n",
       " '四',\n",
       " '办公室和工厂均位于新舄县新舄市秋叶区南町。',\n",
       " '这样人类食用此植物即可使人体对此疫苗的免疫力。',\n",
       " '政客的特点就是',\n",
       " '是',\n",
       " '托波也是南加州大学的创作计划之毕业生。',\n",
       " '圣阿基兰德帕西人口变化图示',\n",
       " '沼蛙为赤蛙科水蛙属的两栖动物。',\n",
       " '幡状云在沙漠地区极为常见。',\n",
       " '这一突如其来的毒素提升是一个快速累积的例子。',\n",
       " '这个游戏是个能够找出最适合询问玩家的问题并学习的人工智能程序。',\n",
       " '桂文耀人。',\n",
       " '安徽大学法学专业本科毕业。',\n",
       " '它分唇内和唇外两种。',\n",
       " '川西无心菜',\n",
       " '滇缅斑鸠菊是菊科班鸠菊属的植物。',\n",
       " '五角龙因拥有陆地脊椎动物中最大型的头颅骨而著名。',\n",
       " '百股街道是中国辽宁省锦州市凌河区下辖的一个街道。',\n",
       " '曾发生内阁签订伦敦海军条约引起干犯统帅权的重大争议。',\n",
       " '游戏沿用典型日式角色扮演游戏设定。',\n",
       " '此时必须通报警察才能解除闪红灯。',\n",
       " '唐高祖太穆窦皇后的叔父。',\n",
       " '确山县城内有进士坊为张守约立。',\n",
       " '总部设在巴黎。',\n",
       " '撤销武汉军区空军番号。',\n",
       " '计算化学这个名词有时也用来表示计算机科学与化学的交叉学科。',\n",
       " '列岛北起东引乡北岸。',\n",
       " '坦帕湾区也曾是许多小联盟球队的主场。',\n",
       " '出现会风灾。',\n",
       " '现任的总领事乃南杰瑞。',\n",
       " '他的母亲八岁时因产下其兄弟时所患的并发症而去世。',\n",
       " '乘客也可使用手机扫码支付乘车。',\n",
       " '从各地来的参赛者将于各种地方在主持人乃哥的指示下完成各项挑战或遭到淘汰。',\n",
       " '泰安县著名贪官。',\n",
       " '副作用包含嗜睡和晕眩。',\n",
       " '黑道之家的家庭与黑道中猜疑与背叛的故事。',\n",
       " '光绪三十年由张之洞选送入京师大学堂。',\n",
       " '现实生活说明我们错了。',\n",
       " '该计划已有先例。',\n",
       " '我会再来看妳',\n",
       " '斯洛维尼亚运动员曾在夏季奥会的七种运动及冬季奥运的五种运动获得奖牌。',\n",
       " '武氏玉源是海阳处唐安县郿墅社三月二十一日出生。',\n",
       " '从这季开始由执行监制接替前两季的担任新的节目统筹。',\n",
       " '皱叶毛建草为唇形科青兰属下的一个种。',\n",
       " '稀代樱子，日本资深女性配音员。',\n",
       " '嘉靖三十八年，登进士第二甲第四十七名。',\n",
       " '大叶石岩枫为大戟科野桐属下的一个变种。',\n",
       " '努尔曼别提是吉尔吉斯斯坦楚河州伊塞克阿塔区下辖的一个村。',\n",
       " '导致信号衰落的信道被称作衰落信道。',\n",
       " '南直隶松江府上海县人。',\n",
       " '米纳维尔莱班加尔。',\n",
       " '若底面为正方形则称为正四角锥反角柱。',\n",
       " '腺顶突吸虫为微茎科顶突属的动物。',\n",
       " '球场拥有一个可供新加坡职业足球联赛举办赛事的草皮。',\n",
       " '莱斯穆蒂耶尔于贝尔。',\n",
       " '圣热纳维耶夫。',\n",
       " '建构主义是被用来解释国际关系的一种新方式。',\n",
       " '河流流经。',\n",
       " '明治时代称为赞岐街道。',\n",
       " '他在高中时代投过六场无安打比赛。',\n",
       " '担任南京水利科学研究院副总工程师。',\n",
       " '博尔赫斯视这部写于其晚年失明时的集子是他最好的作品。',\n",
       " '爸爸我担心的要死。',\n",
       " '豪瑟梅尔恩是德国图林根州的一个市镇。',\n",
       " '三',\n",
       " '六',\n",
       " '靴筒亦能保护策骑者的腿部避免与马鞍磨擦。',\n",
       " '最终定名为山东省烟台第一中学。',\n",
       " '谢夫里耶尔人口变化图示',\n",
       " '氟化锌是锌最常见的氟化物。',\n",
       " '该馆的创办者是画家何扬夫妇。',\n",
       " '因此花不弃和陈煜的感情之路坎坷曲折。',\n",
       " '狠狠甩了她两个耳光',\n",
       " '后居民迁出。',\n",
       " '他因此而引起观众注意。',\n",
       " '脚注书目',\n",
       " '殿试登进士第二甲第十四名。',\n",
       " '同时也是一名医生。',\n",
       " '为平成假面骑士系列的第五系列录影带首映作品。',\n",
       " '未有中文书名以日文原名表示。',\n",
       " '多斯帕洛斯怀是位于美国加利福尼亚州默塞德县的一个人口普查指定地区。',\n",
       " '它被用于食品工业。',\n",
       " '曾祖父毛彦仁。',\n",
       " '所以厂商有时会附上能制作可开机还原光碟的软体以便使用者能备份还原资料。',\n",
       " '出云郡是过去属于出云国的郡。',\n",
       " '西圣玛丽亚是巴西巴拉那州的一个市镇。',\n",
       " '张朝发墓的历史年代为清。',\n",
       " '印第奥是美国加利福尼亚州里弗赛德县的一座城市。',\n",
       " '手洞附近的一个山洞。',\n",
       " '余英时认为程智为商人哲学之代表者。',\n",
       " '杨西昆表示此方案需要美国支持并说服蒋介石。',\n",
       " '不开放团体预约',\n",
       " '位于洛杉矶的图书馆及数据库已经收藏近五万卷藏书及非印刷类资料。',\n",
       " '福赛斯县是美国乔治亚州北部的一个县。',\n",
       " '尽管芭芭拉也因此获得卡素尔勋爵夫人。',\n",
       " '云端应用本地应用',\n",
       " '网友可以在线阅读和下载这些文档。',\n",
       " '有人认为是未来的龙妃。',\n",
       " '国家生活中有了一些改良的措施。',\n",
       " '巴赫为处理独奏乐器和乐队提供了不同方式。',\n",
       " '被誉为中国水蜜桃之乡。',\n",
       " '获勋者可以免费乘坐公共交通。',\n",
       " '维尔图努斯古罗马神话神只之一。',\n",
       " '导致更多的薛延陀人相信唐朝主力已经抵达。',\n",
       " '而垂耳鸦科下的物种分布于新西兰。',\n",
       " '希莱维的职业生涯主要在沙特阿拉伯国内的伊蒂哈德俱乐部度过。',\n",
       " '死火山即已经长期没有了爆发迹象的火山。',\n",
       " '先驱者金星计划两个探测器组成。',\n",
       " '莫朗上布勒蒂尼的总面积为平方公里。',\n",
       " '名称等以废除时以准。',\n",
       " '那时的面包只是发酵的粮食饼。',\n",
       " '海洋与河流干涸',\n",
       " '三肋果属是菊科下的一个属。',\n",
       " '主题曲由文千岁主唱。',\n",
       " '是',\n",
       " '七',\n",
       " '现任主席为叶国谦。',\n",
       " '节目于逢星期六晚上七时半播映。',\n",
       " '他属于胡林家族。',\n",
       " '毛粗丝木为茶茱萸科粗丝木属下的一个种。',\n",
       " '担任工商界人士。',\n",
       " '长果砂仁为姜科豆蔻属下的一个种。',\n",
       " '民众们也能观赏战况。',\n",
       " '家庭工作的奴隶在非洲很多地方都有。',\n",
       " '他现在效力于英格兰足球超级联赛球队伯恩利足球俱乐部。',\n",
       " '曾任中国青年党中央党部秘书长。',\n",
       " '工人民主党是土耳其的一个托洛茨基主义政党。',\n",
       " '科诺瓦洛沃市镇是俄罗斯联邦伊尔库茨克州巴拉甘斯克区所属的一个市镇。',\n",
       " '塔的动态特性也是在设计时需考虑的限制之一。',\n",
       " '改任常德府知府。',\n",
       " '游戏收到混合至平均的评价。',\n",
       " '她在刘曜即位前就去世了。',\n",
       " '在能力范围内',\n",
       " '程琳人。',\n",
       " '毕业于吉林大学世界经济专业。',\n",
       " '有完整的前缘脉及明显的亚前缘脉。',\n",
       " '按钮的形状通常是圆形或方形。',\n",
       " '代谢生化学是研究生物体内化学物质的利用消耗以及生物体所需分子的合成。',\n",
       " '新疆亚菊是菊科亚菊属的植物。',\n",
       " '钱楚拉是一个位于美国阿拉巴马州莫比尔县的非建制地区和人口普查指定地区。',\n",
       " '养育巷站位于苏州市姑苏区的干将西路的干将河下。',\n",
       " '青帘贳酒今何少？',\n",
       " '巴彦淖尔庙位于昌图锡力苏木巴彦淖尔嘎查境内。',\n",
       " '随后在天津的一个政府部门工作。',\n",
       " '礼仪部分用五礼为顺序。',\n",
       " '大张伟',\n",
       " '采取互递归定义的最重要的基本数据类型是树。',\n",
       " '以下是古河三水会的成员。',\n",
       " '极少数还会发生颜面神经麻痹。',\n",
       " '他们并不适合有小孩的家庭。',\n",
       " '七',\n",
       " '三',\n",
       " '简式威妥玛的威妥玛罗马拼音系统的修改版本。',\n",
       " '不是让你拿来玩的',\n",
       " '台下亦设有多个小食摊档。',\n",
       " '组合博弈一般是动态博弈。',\n",
       " '是',\n",
       " '六',\n",
       " '主炮使用的高爆弹能有效对付牵引式火炮和步兵。',\n",
       " '汉堡排是一种由牛绞肉或其他夹有牛绞肉做成的肉饼。',\n",
       " '嘉庆五年中式庚申恩科江南乡试举人。',\n",
       " '历任国家科委局长。',\n",
       " '教区位于玻利瓦尔省。',\n",
       " '八大省工是指八所台湾著名的工业教育学校。',\n",
       " '她是托勒密十二世唯一确认的妻子。',\n",
       " '阿库尔纳瓦卡特尔是阿斯卡波特萨尔科之统治者之子。',\n",
       " '即练习膀臂颤抖所产生的劲道。',\n",
       " '其他的则有警察的阶级等等。',\n",
       " '可食用或做观赏鱼。',\n",
       " '柄囊蕨为鳞毛蕨科柄囊蕨属下的一个种。',\n",
       " '父亲是高远满继。',\n",
       " '国民革命成功后曾任中山大学副校长。',\n",
       " '文塞斯劳总统镇是巴西圣保罗州的一个市镇。',\n",
       " '亲切又慈祥的呵护著',\n",
       " '王观辽国政治家。',\n",
       " '埃格尔城堡是位于匈牙利城市埃格尔附近的一座城堡。',\n",
       " '五',\n",
       " '一',\n",
       " '一',\n",
       " '八',\n",
       " '列车的软卧车和餐车都装有液晶电视。',\n",
       " '商场上为服务设施大楼。',\n",
       " '总部设在秘鲁首都利马。',\n",
       " '张延寿人。',\n",
       " '三',\n",
       " '否',\n",
       " '小施瓦布豪森是德国图林根州的一个市镇。',\n",
       " '体香剂一般采用喷雾喷射或者走珠涂抹的方式进行使用。',\n",
       " '薪资上涨很不方便',\n",
       " '布雷塞是德国勃兰登堡州的一个市镇。',\n",
       " '此种观点视通货膨胀为对未来资本价值的不确定性。',\n",
       " '这组寺庙群后毁于八国联军之手。',\n",
       " '零',\n",
       " '三',\n",
       " '沙尔克蒙。',\n",
       " '其中有三分之二在新英格兰居住。',\n",
       " '津浦路西沽机场旧址是原天津市机车车辆修理的重要基地。',\n",
       " '大部分碑文包含人名。',\n",
       " '虚拟集线器就是以软体方式模拟网路交换机。',\n",
       " '它设于新加坡植物园北部。',\n",
       " '没有人迹的山丘',\n",
       " '不带镇流器的折叠形光管称紧凑型荧光灯。',\n",
       " '位于连接首都莫斯科和乌克兰首都基辅以及顿河畔罗斯托夫的铁路上。',\n",
       " '买了好多巧克力当伴手礼',\n",
       " '尊韩山童妻杨氏为皇太后。',\n",
       " '其中霸王色霸气能够搭配叫声产生实质的破坏力。',\n",
       " '和政县是中华人民共和国甘肃省临夏回族自治州下属的一个县。',\n",
       " '本届进博会参展销售的所有进口展品同时实行税收优惠政策。',\n",
       " '克雷西库韦。',\n",
       " '中国戏曲音乐方面是综合了古代的词曲音乐以及民间歌曲发展而成的。',\n",
       " '零',\n",
       " '六',\n",
       " '大部分被称为巴布亚人。',\n",
       " '这些职业也都出现在之后的很多最终幻想作品中。',\n",
       " '静电敏感设备。',\n",
       " '吉普瑟姆镇区为美国堪萨斯州塞奇威克县辖下的镇区。',\n",
       " '位于东京都大田区北部。',\n",
       " '他以征伐楚科奇人而知名。',\n",
       " '是',\n",
       " '五',\n",
       " '社会语言学的研究内容与语用学非常相似。',\n",
       " '染发剂也可以覆盖在头皮上以获得更坚实的层面。',\n",
       " '信徒会年年去沤汪文衡殿请火。',\n",
       " '耶律尧骨没有攻打易州城。',\n",
       " '后返回京师负责各道的奏章。',\n",
       " '本鱼分布于全世界暖温带海域。',\n",
       " '全缘叶蓝刺头为菊科蓝刺头属的植物。',\n",
       " '苏宁的骨灰安放于哈尔滨烈士陵园。',\n",
       " '授东昌府堂邑县丞。',\n",
       " '溯北随县曾侯乙墓中曾出土。',\n",
       " '脂类组学是生物系统中细胞脂类途径和网络的大规模研究。',\n",
       " '有很多传统习俗和过年活动',\n",
       " '圣莫尼卡是巴西巴拉那州的一个市镇。',\n",
       " '李阳所配音的广告在香港和东南亚电视台广泛播送。',\n",
       " '广州市中山大学哲学系教授。',\n",
       " '库柏出生于伦敦的格林尼治。',\n",
       " '只设北行入口与南行出口。',\n",
       " '动态组字是一种汉字在电脑等领域的编码理论及技术。',\n",
       " '日本航空的最大股东。',\n",
       " '楚措拉是罗马尼亚雅西县的一个镇。',\n",
       " '蟹笼是捕捞红眼雪蟹的主要方法。',\n",
       " '多摩川线的路线名曾多次更改。',\n",
       " '迈纳是一个位于美国阿拉巴马州杰佛逊县的非建制地区和人口普查指定地区。',\n",
       " '尼德罗。',\n",
       " '零',\n",
       " '六',\n",
       " '五棵松因早年此地有五棵松树而得名。',\n",
       " '艰难梭菌感染在全世界都可能发生。',\n",
       " '由大温哥华区域局管理。',\n",
       " '有一天你会明白',\n",
       " '山东乡试第六名。',\n",
       " '梁思庄是梁启超的次女。',\n",
       " '博斯克贝纳尔科曼人口变化图示',\n",
       " '贝朗格。',\n",
       " '该片在鹿特丹电影节和法兰德斯电影节均获最佳影片奖。',\n",
       " '借由艺术作品',\n",
       " '今日曹甸镇以生产文体教玩具着称。',\n",
       " '马尔萨克人口变化图示',\n",
       " '邓通自始得到汉文帝的欢心。',\n",
       " '双边贸易和投资近年来正在不停地增长。',\n",
       " '虽然时间和疏于照管使废墟受到了破坏。',\n",
       " '寺内的三塔已被列为全国重点文物保护单位。',\n",
       " '他在这段期间的作品受到了狩野派代表的强烈影响。',\n",
       " '也指在被认定为绝灭之后又再次发现的物种。',\n",
       " '他发现并采集了大量南非和非洲热带的植物标本。',\n",
       " '现存关于银星勋章的规定位于美国法典的第十卷。',\n",
       " '杜阿拉联合足球俱乐部',\n",
       " '东方狼蛛为狼蛛科狼蛛属的动物。',\n",
       " '传世墨拓善本有元代赵孟俯藏本。',\n",
       " '农历五月十三日是关平帝君诞。',\n",
       " '沱江镇自清朝开始就一直是凤凰县的行政中心。',\n",
       " '旁白为黄启昌。',\n",
       " '三',\n",
       " '四',\n",
       " '父亲程景和。',\n",
       " '密苏里领地的首府为圣路易斯。',\n",
       " '六',\n",
       " '四',\n",
       " '长男是演员出光秀一郎。',\n",
       " '山东省政府驻地设在城内原伪省长公署院内。',\n",
       " '麦克阿瑟将军则认为这次事件是意图推翻美国政府的行为。',\n",
       " '该物种的模式产地在贡山县独龙。',\n",
       " '清朝以工部掌其事。',\n",
       " '目前办理旅客运输业务及货运业务。',\n",
       " '瓦雷斯内。',\n",
       " '在行政院农业委员会渔业署是归类为第二类渔港。',\n",
       " '科氏长须鲨是卵胎生的。',\n",
       " '部分火车乘客被困达三到五个小时。',\n",
       " '他也提出了适者生存的生物理论。',\n",
       " '用户还可以评论他们附近的其他人的贡献。',\n",
       " '生于浙江鄞县走马塘陈家。',\n",
       " '被誉为晋剧皇后。',\n",
       " '九',\n",
       " '零',\n",
       " '库里马塔是巴西皮奥伊州的一个市镇。',\n",
       " '红钟杜鹃为杜鹃花科杜鹃属下的一个种。',\n",
       " '六',\n",
       " '是',\n",
       " '赤竹为禾本科赤竹属下的一个种。',\n",
       " '东北部突出而西南部平缓。',\n",
       " '万历年间任福建巡抚。',\n",
       " '只有印第安纳和马里昂因为闭上眼睛而存活下来。',\n",
       " '曾祖父李成。',\n",
       " '灰色青荚叶为山茱萸科青荚叶属下的一个变种。',\n",
       " '我还是先回家好了',\n",
       " '这种数据通常是表单中的一个数据项。',\n",
       " '门神是木版年画的主要题材之一。',\n",
       " '视动反射是扫视和追随平稳运动的组合。',\n",
       " '大邱庄镇是中国天津市静海县下辖的一个镇。',\n",
       " '经森林防护收集数据后会传送到监察系统进行分析。',\n",
       " '食肉牛龙生活于上白垩纪所描述及命名。',\n",
       " '日期未标注地区者皆为日本之发售日期。',\n",
       " '九',\n",
       " '是',\n",
       " '对于许多国际物流网络来说航空运输是一个关键环节。',\n",
       " '为松坂大辅的前辈。',\n",
       " '应天府乡试第七十四名。',\n",
       " '大叶马兜铃是马兜铃科马兜铃属的植物。',\n",
       " '男爵是中国古代封建制度五等爵中的一个等级。',\n",
       " '少数植株网纹极其丰富。',\n",
       " '卡通人生是英国歌手米卡的首张录音室专辑。',\n",
       " '此路线在天水围的车站位置与线相同。',\n",
       " '嘉靖四十四年，登进士第三甲第六十八名。',\n",
       " '正方走出家门，看见一个有刺青的流氓在家门口前在乱丢垃圾，却对他敢怒不敢言。',\n",
       " '奥迪北区股份有限公司',\n",
       " '她也希望她能平平安安地长大',\n",
       " '巴拿马运河大西洋一侧的终点港克里斯托瓦尔和科隆就位于利蒙湾东岸。',\n",
       " '胸导管是人和羊膜动物体中最长最大的淋巴管。',\n",
       " '四',\n",
       " '是',\n",
       " '本次比赛由中国连续第二次在中华台北决赛中获胜。',\n",
       " '罗威·迪恩是一名加拿大男导演、制片人、编剧、剪辑师和演员。',\n",
       " '近来人们也开始陆续关注其他无线电子设备及系统对人体健康的影响。',\n",
       " '疏叶丝带藓为蔓藓科丝带藓属下的一个种。',\n",
       " '埃波特罗莱。',\n",
       " '杉津停车区是位于福井县敦贺市的北陆自动车道之休息站。',\n",
       " '此后全省第一家外资商场八佰伴又进驻中山路南段。',\n",
       " '短吻前肛鳗的图片',\n",
       " '维新者何政？',\n",
       " '分配系数为某物质的未解离形态在两相中的活度之比。',\n",
       " '麦考夫声称尤莉丝绝不可能逃出。',\n",
       " '嘉靖三十五年丙辰科二甲进士。',\n",
       " '首播重播',\n",
       " '近铁日本桥站的铁路车站。',\n",
       " '对于量子理论作出相当大的贡献。',\n",
       " '基要派是与自由派神学相对立的。',\n",
       " '费埃罗奈埃。',\n",
       " '汐止会是台湾三大犯罪组织之一天道盟天鹰会旗下分会。',\n",
       " '三',\n",
       " '六',\n",
       " '警方第二天扣押了车辆。',\n",
       " '全民及专业投票组别分数各占一半。',\n",
       " '平时说话很害羞且十分之维护佛莱德。',\n",
       " '最终以袁术及其仲氏政权败亡而告终。',\n",
       " '上杉氏家臣。',\n",
       " '工业酒精含有少量有毒性的甲醇。',\n",
       " '他与京都粟田口派铸剑师有关系。',\n",
       " '阿方索二世从父亲那里继承了一个切实可行的防御计划。',\n",
       " '使用族群为菲律宾华人。',\n",
       " '戴隆邦传戴隆邦之妻侄郭维汉。',\n",
       " '随后几年内延长至来广营。',\n",
       " '塔上设有瞭望台与旋转餐厅。',\n",
       " '排名依奖牌总数依总成绩及单项成绩',\n",
       " '毕业后长期在军事科学院从事军事理论研究。',\n",
       " '该陨坑与北侧一对更小的撞击坑一起形成一组相连的坑簇。',\n",
       " '双亭桥将景区分为南苑和北苑。',\n",
       " '曲茎假糙苏为唇形科假糙苏属下的一个种。',\n",
       " '由权利人投保义务人的信用风险。',\n",
       " '布兰登则表示他必须重新得到眼来完成书。',\n",
       " '早年曾师从德籍化学师专攻化学工业。',\n",
       " '吕海寰西南隅村人。',\n",
       " '泰始二年兵溃逃还寿阳。',\n",
       " '第十三届全国人大一次会议通过了宪法修正案。',\n",
       " '四星学园的男生偶像团体。',\n",
       " '微分域是微分伽罗瓦理论中的研究对象。',\n",
       " '它被看作是西欧最古老的城市。',\n",
       " '生于奉化市。',\n",
       " '米斯特拉斯是希腊同名村庄附近的一座古代拜占庭废墟城市。',\n",
       " '维托人口变化图示',\n",
       " '浙江乡试第二十六名。',\n",
       " '目前日治时期的台南病院仍有遗构位在隔壁的台南护专里面。',\n",
       " '苏丹阿都阿兹皇家展览馆的收藏。',\n",
       " '大楼的高度和字母相关。',\n",
       " '并发症包括自杀。',\n",
       " '作为冶金和化学生产量大的结果使得切列波韦茨是世界上污染最严重的城市之一。',\n",
       " '航线于开办时是由油麻地小轮营办。',\n",
       " '七',\n",
       " '三',\n",
       " '隆庆五年，登进士第三甲第一百七十名。',\n",
       " '地球由于大规模战争和环境破坏导致冰河期来临，迫使人类移居到地下。',\n",
       " '贝斯蒂阿克人口变化图示',\n",
       " '在市场里面',\n",
       " '而后陆续在部分数位有线电视系统上架。',\n",
       " '束花石斛为兰科石斛属下的一个种。',\n",
       " '亲子鉴定亦为医检师的工作内容之一。',\n",
       " '绍卡伊。',\n",
       " '帕尔马及皮亚琴察公爵恩里克一世。',\n",
       " '酂城镇是中国河南省永城市下辖的一个镇。',\n",
       " '此站是船桥市内位处最北端的车站。',\n",
       " '在挹娄故地。',\n",
       " '这本篇幅不大的书提纲挈领地讲述了当时的代数学知识。',\n",
       " '东南面的太平洋季候风会带来充沛的雨量以及热带气旋活动。',\n",
       " '汉朝时为汝南郡地。',\n",
       " '祖父王义刚。',\n",
       " '晋果败子玉于城濮。',\n",
       " '米洛舍维奇派志愿军到克罗埃西亚境内援助塞尔维亚族人。',\n",
       " '洲际公路通过伊普萨拉靠近马尔马拉海的边境检查点。',\n",
       " '语义网络常常用作知识表示的一种形式。',\n",
       " '死六臣之一。',\n",
       " '辽肃祖耶律耨里思妻。',\n",
       " '永远不会疲倦的旅程',\n",
       " '圣索尔兰德维安。',\n",
       " '零',\n",
       " '七',\n",
       " '圣雅克路是巴黎拉丁区的一条街道。',\n",
       " '法名净安。',\n",
       " '这使公园成为城市中最受欢迎的地方。',\n",
       " '粟屋站三江线的铁路车站。',\n",
       " '古印第安人慢慢适应新的环境。',\n",
       " '具体日期取决于他们居住的位置和投票的方式。',\n",
       " '道路顺序由北往南排列地铁',\n",
       " '目的是促进国际间的数学交流与合作。',\n",
       " '曾任河北梆子剧院一团团长。',\n",
       " '蓬和马塞恩。',\n",
       " '官至华盖殿大学士。',\n",
       " '雷锋的中国人民解放军中士及共产主义战士。',\n",
       " '七',\n",
       " '六',\n",
       " '大多数绍尔茨人信仰天主教。',\n",
       " '受牵连的十六位御史均被贬为典史。',\n",
       " '中华人民共和国成立后被聘为上海市文史馆馆员。',\n",
       " '表面电荷即在界面处存在的电荷。',\n",
       " '小品和相声在表现形式上有非常大的不同。',\n",
       " '这也是前赵的第一个年号。',\n",
       " '虎丘古迹中最著名的是云岩寺塔和剑池。',\n",
       " '狭基线纹香茶菜为唇形科香茶菜属下的一个变种。',\n",
       " '奥斯曼帝国统治时期这里称为哈勒普。',\n",
       " '吴忌寒推特吴忌寒微博',\n",
       " '普吕纳雷。',\n",
       " '下阿姆。',\n",
       " '本剧由释由美子主演。',\n",
       " '维拉尔雷蒙。',\n",
       " '圣马尔蒂阿昂特赖盖。',\n",
       " '其后萧乐古下场不详。',\n",
       " '拉里沙尔代人口变化图示',\n",
       " '其父为当地中学教师。',\n",
       " '引导少讯会员互相激励打击青少年罪案。',\n",
       " '孔灵龟之子孔硕是孔颖达的祖父。',\n",
       " '该建筑现为天津民俗博物馆。',\n",
       " '梦东方集团有限公司。',\n",
       " '乌柿为柿科柿属下的一个种。',\n",
       " '该队曾多次参加欧洲冠军联赛。',\n",
       " '但如果只知道容器在漏完水后的某个时刻的状态。',\n",
       " '白毛华丽杜鹃为杜鹃花科杜鹃属下的一个变种。',\n",
       " '车站坐落于安徽省芜湖市镜湖区两站广场。',\n",
       " '邦尼拜的足球生涯在家乡球会以青年球员的身份展开。',\n",
       " '主要以观赏人次为主，票房为辅。',\n",
       " '罗恩·贾金斯，美国、编剧和导演。',\n",
       " '其中团体组的部份选手有可能是评委会从落选的个人选手中组合而成。',\n",
       " '尔利效应也会跟着改变。',\n",
       " '二',\n",
       " '六',\n",
       " '旁边就有哈密瓜',\n",
       " '最终生产出了不逊于市场同级发动机指标的产品。',\n",
       " '同时，少女模特儿的存在在动漫爱好者的参观人士当中也引起不少争议。',\n",
       " '圣依纳爵堂，古典主义风格。',\n",
       " '乡原古统教美术课程',\n",
       " '叶定仕故居已被古物古迹办事处评为香港法定古迹。',\n",
       " '每次见面都笑得像孩子般',\n",
       " '他也代表塞尔维亚各级国青队参赛。',\n",
       " '七叶一枝花为百合科重楼属的植物。',\n",
       " '镇政府驻横沟市。',\n",
       " '两浙西路是两宋时期的一个地方行政区。',\n",
       " '武藏人口变化图示',\n",
       " '小崎亚衣的哥哥是漫画家。',\n",
       " '下桑坦村是位于美国亚利桑那州皮纳尔县的一个人口普查指定地区。',\n",
       " '湖南善化县人。',\n",
       " '因根里德是德国巴伐利亚州的一个市镇。',\n",
       " '结果差的整体选择性就被观测到了。',\n",
       " '唐朝至德二载改安康郡置汉南郡。',\n",
       " '解散直前组织。',\n",
       " '次子乌巴岱。',\n",
       " '所有元素都有补元的有界格叫做有补格。',\n",
       " '这与女权运动不无关系。',\n",
       " '不是所有语言都可被轻易的分类为完全属于多式综合语。',\n",
       " '是海螂目鸥蛤科铃蛤属的一种。',\n",
       " '类思堂继续开放。',\n",
       " '当以正常速度重播时，时间似乎变得更慢。',\n",
       " '日本的妈妈',\n",
       " '曾祖父刘兴。',\n",
       " '由游戏设计者所创造。',\n",
       " '此外还有一些其他情况下的类似的论点都没有得到法院的支持。',\n",
       " '北伐时期朱德的参谋长兼秘书。',\n",
       " '鹰司的名字出自平安京的鹰司小路。',\n",
       " '九',\n",
       " '八',\n",
       " '勋略是长方形以红色作底色而左右两方左右各有一条黄色粗线毗邻。',\n",
       " '片仓喜多是日本战国时代的伊达家家中女性。',\n",
       " '陈家持续管理其集团扩大其事业。',\n",
       " '台湾与日本关系史是指台湾历代与日本历代在历史上不同阶段的关系。',\n",
       " '山上开始下雪',\n",
       " '他经常被约旦当局关押。',\n",
       " '官至侍读学士。',\n",
       " '装一袋到袋子里',\n",
       " '这种方法比重新输入所有文案快很多。',\n",
       " '马绍尔群岛以马绍尔群岛奥林匹克委员会身份参加奥林匹克运动会。',\n",
       " '尾叶耳蕨为鳞毛蕨科耳蕨属下的一个种。',\n",
       " '担任中国三江航天集团总经理。',\n",
       " '抚松乌头为毛茛科乌头属下的一个种。',\n",
       " '其子义辰则于侍奉后任的伊予松山藩主松平氏时因故被贬为浪人。',\n",
       " '格罗森克内滕是德国下萨克森州的一个市镇。',\n",
       " '蒂尔斯泰因是德国巴伐利亚州的一个市镇。',\n",
       " '参见秦朝官制。',\n",
       " '他选择诺定咸森林因为他觉得诺定咸森林会有更大的发展机会。',\n",
       " '其他于开拓动漫祭期间外进行的关联活动。',\n",
       " '他现在效力于义大利甲级联赛球队桑普多利亚足球俱乐部。',\n",
       " '本尊法然上人像为法然上人。',\n",
       " '大眼蝠属等之数种哺乳动物。',\n",
       " '阿姆斯特丹大学还拥有五个博物馆。',\n",
       " '布兰德是德国巴伐利亚州的一个市镇。',\n",
       " '清朝乾隆三十六年重修。',\n",
       " '与该口岸相接的是中国广西的龙邦口岸。',\n",
       " '军龄从毕业后算起。',\n",
       " '在接着两季他成为罗连安特的核心球员并助球队升班甲组。',\n",
       " '且这时的扫盲中的政治教育比例已比较重。',\n",
       " '雅各的同父母的侄子。',\n",
       " '吴椿，字寿卿，江西南昌府新建县人，民籍，明朝政治人物、进士出身。',\n",
       " '本案主要的争议是冷冻胚胎的法律地位。',\n",
       " '九',\n",
       " '零',\n",
       " '那对新加坡新人',\n",
       " '马爹利选择纹理细密的橡木桶进行最后一步的熟成。',\n",
       " '因此操作系统的实现在很多地方往往用自旋锁。',\n",
       " '数位电视的转换即释出了许多频宽。',\n",
       " '耶稣回到加利利是在施洗约翰被逮捕后。',\n",
       " '近几年获奖无数。',\n",
       " '他是苏格兰特许会计师公会会员及香港会计师公会会员。',\n",
       " '高等民事法院第一分部一致推翻了以下决定。',\n",
       " '隶属于安徽省教育厅。',\n",
       " '该校原名山东蓝翔高级技工学校。',\n",
       " '祖父张礼。',\n",
       " '国除为淮南郡。',\n",
       " '所有月台都设有月台幕门。',\n",
       " '让我试一下嘛！',\n",
       " '失剌比是是印度尼西亚苏门答腊西南海岸的港口小镇。',\n",
       " '多毛铃子香为唇形科铃子香属下的一个种。',\n",
       " '北魏孝昌二年。',\n",
       " '贞观二年废。',\n",
       " '追赠赠金州刺史。',\n",
       " '克图格亚中。',\n",
       " '五',\n",
       " '四',\n",
       " '我们这些艾格尔顿好公民确实存在。',\n",
       " '雅礼协会致力于开展多种形式的教育项目来达成这一使命。',\n",
       " '她的父母都有日本血统。',\n",
       " '尚未成为现道的绕道以神奈川县最多。',\n",
       " '目前是台湾现役唯一一位女子雪橇选手。',\n",
       " '魏孝文帝元宏娶韦崇的女儿为充华嫔。',\n",
       " '勒洛勒。',\n",
       " '升左春坊左中允。',\n",
       " '亚历山大县是位于美国北卡罗莱纳州西部的一个县。',\n",
       " '沙朗塞。',\n",
       " '沙漠景观高地是位于美国加利福尼亚州洛杉矶县的一个人口普查指定地区。',\n",
       " '当时以藤原氏为代表的贵族阶层流行观想念佛。',\n",
       " '窃衣是伞形科窃衣属的植物。',\n",
       " '后裂小副克里介为荆花介科小副克里介属下的一个种。',\n",
       " '其他两个克罗内克函数则要求行与列的指标必须相等才能得到一个非零的结果。',\n",
       " '卡克塔省。',\n",
       " '尚布里人口变化图示',\n",
       " '上林湖越窑遗址是中国浙江省慈溪市境内的一处古代瓷窑遗址。',\n",
       " '吴瑶之子。',\n",
       " '犬齿后方的牙齿宽广。',\n",
       " '每天陪她到医院做复健',\n",
       " '该市镇年时的人口为人。',\n",
       " '由昌能继承家督之位。',\n",
       " '齐亚诺被委派为阿尔巴尼亚总督。',\n",
       " '组建后该军事部一直借住长白山科学研究院原办公楼。',\n",
       " '两族坟山讼案就此了结。',\n",
       " '科兹洛沃村委员会是俄罗斯联邦阿斯特拉罕州沃洛达尔斯基区所属的一个村委员会。',\n",
       " '缔结姊妹市',\n",
       " '这可以用二次打击假说来解释。',\n",
       " '妻子为香港行政会议成员胡红玉。',\n",
       " '最早的砖造大型教堂是索勒隐修院教堂。',\n",
       " '崔家集镇是中国山东省青岛市平度市下辖的一个镇。',\n",
       " '莱斯特广场的广场。',\n",
       " '肽聚糖中的糖链由乙酰氨基葡萄糖和乙酰胞壁酸交联聚合而成。',\n",
       " '这年夏天梵高爱上了新寡的表姐凯到埃滕花园同住。',\n",
       " '他们在此地建设大型的别墅。',\n",
       " '日治时期',\n",
       " '东接一番町。',\n",
       " '这宗空难是同时涉及美军军机空难中最致命的空难。',\n",
       " '警方在他车上搜出一把半自动手枪。',\n",
       " '可作为酰化剂对富电子活化芳环进行酰化反应。',\n",
       " '中国科学院电子学研究所研究员。',\n",
       " '六',\n",
       " '是',\n",
       " '新加坡社会及家庭发展部是新加坡政府的一个下辖部门。',\n",
       " '龙鹰如何保住大唐最后的希望？',\n",
       " '演员名单来自。',\n",
       " '还有起司蛋糕',\n",
       " '每年的主题都会对正式和非正式的教育工作者免费的教育计画支援。',\n",
       " '中国金鹰电视艺术节承办的中国大陆电视艺术节庆活动。',\n",
       " '以低山为主。',\n",
       " '任湄云舰。',\n",
       " '如无机酸中的硫酸等。',\n",
       " '昭公即位后安葬被杀的人。',\n",
       " '它最早与族群。',\n",
       " '歌舞伎一般普遍使用。',\n",
       " '六',\n",
       " '二',\n",
       " '仇陆和港独的思潮已在香港蔓延。',\n",
       " '幸町是北海道上川地方上川郡下川町的一个地名。',\n",
       " '埃皮耶德。',\n",
       " '现行行政地名为大藏一丁目至六丁目。',\n",
       " '真正的治疗不是这样',\n",
       " '会议亦批准中华人民共和国及台湾分别加入世界贸易组织。',\n",
       " '重庆大学汽车工程学院汽车设计与制造专业毕业。',\n",
       " '该物种的模式产地在印度。',\n",
       " '由此等可快速前往台湾西部各地。',\n",
       " '否则只是专业人士对一般老百姓的的知识诈欺。',\n",
       " '想占有的东西就是想占有',\n",
       " '它也是突厥斯坦的一部分。',\n",
       " '米申是美国的一座城市。',\n",
       " '手碟是在对钢鼓和其他乐器的多年研究的基础上创造的。',\n",
       " '彭道传。',\n",
       " '佩和代人口变化图示',\n",
       " '长果婆婆纳为车前草科婆婆纳属下的一个种。',\n",
       " '祭品文更是不胜枚举',\n",
       " '车站位于神墩一路与光谷六路交叉路口处，沿神墩一路东西向布置。',\n",
       " '在中国鸦片泛滥的年代，不同材质的烟枪甚至成为了身份和地位的象征。',\n",
       " '我们跟著人群走',\n",
       " '本鱼是一种雌性先熟的雌雄同体。',\n",
       " '还是觉得开口太紧了',\n",
       " '属于起初在阿尔泰牧居的哈萨克阿巴克克烈部落的切如齐部落。',\n",
       " '它们有着很多鸟类的特征。',\n",
       " '永泰元年任侍中。',\n",
       " '从南北美洲到欧洲大陆的邮船大都在此停泊。',\n",
       " '阿布洛维尔。',\n",
       " '他的生活不容易。',\n",
       " '阿卡德语曾充当古代近东的通用语长达数世纪。',\n",
       " '县治罗拉。',\n",
       " '天赐元年将今浮山地另置葛城县。',\n",
       " '七',\n",
       " '是',\n",
       " '大埔八号餐厅。',\n",
       " '莱泰尔讷。',\n",
       " '鲍德温是出生在南卡罗来纳州的哥伦比亚。',\n",
       " '它也是与西方盟国并肩作战的波军单位后勤核心。',\n",
       " '普雅斯特吕克人口变化图示',\n",
       " '探险家包括了猎人克里顿。',\n",
       " '江宁布政使为从二品。',\n",
       " '祖父高良佐。',\n",
       " '米希达恩的儿时偶像为前皇家马德里领队齐达内。',\n",
       " '天津著名律师朱道孔长期担任报馆的法律顾问。',\n",
       " '这是迄今为止乒超联赛历史上唯一一个男女团参赛队伍数量不相等的赛季。',\n",
       " '曾任中国人民解放军后勤学院后勤组织系主任。',\n",
       " '曾祖父刘文。',\n",
       " '其设计又有分直接吸收和防漏侧边不同的设计。',\n",
       " '斯凯勒县是美国纽约州西部的一个县。',\n",
       " '他先后九次到北京朝见清朝皇帝。',\n",
       " '此次行为遭到业界很多人的质疑。',\n",
       " '由西南流的一级河川是本市象征。',\n",
       " '是',\n",
       " '二',\n",
       " '真水狼蛛为狼蛛科水狼蛛属的动物。',\n",
       " '土卫六大气层是太阳系的天然卫星中唯一发展高度完整的卫星大气层。',\n",
       " '它们因失去栖息地及污染而濒危。',\n",
       " '云南是哈尼族世居地。',\n",
       " '名家的重要人物。',\n",
       " '现任中共天津市委专职副书记。',\n",
       " '天兴六年道武帝封拓跋绍为清河王。',\n",
       " '上海农村商业银行是一家总部设立在上海的法人银行。',\n",
       " '附近有首尔特别市厅。',\n",
       " '我只知道我要找技术的东西',\n",
       " '此站可进行列车交会。',\n",
       " '长苞毛兰为兰科毛兰属下的一个种。',\n",
       " '拉科内特。',\n",
       " '李四光提出的新华夏构造体系中的第二沉降带就是在这时形成的。',\n",
       " '大手印也采用他空见。',\n",
       " '米琼日寺因此而建在这只鸟指示的地址处。',\n",
       " '初中时在美国住过一年。',\n",
       " '若奈人口变化图示',\n",
       " '授湖广麻城县知县。',\n",
       " '升镶红旗汉军都统。',\n",
       " '而马克西米努斯也率军直逼罗马。',\n",
       " '途中雪景又是一绝',\n",
       " '父亲为中华民国外交官何凤山。',\n",
       " '总和生育率的计算方法是某一时间点各年龄别妇女生育率的总和。',\n",
       " '种下分类群使用于三名法中。',\n",
       " '诏安东溪是福建省诏安县的一条河流。',\n",
       " '虎克猪笼草是由苹果猪笼草与莱佛士猪笼草杂交得到的常见的自然杂交种。',\n",
       " '后来一支向尼罗河流域南方扩张的土埃联军征服了这个国家。',\n",
       " '洪氏生于英祖十一年正月十一日与王世子李愃行嘉礼。',\n",
       " '影片以提尔和女朋友在电话中吵架开场。',\n",
       " '三',\n",
       " '二',\n",
       " '这类物质通常是通过让靶细胞因裂解而溶解而起效的。',\n",
       " '首府喀山。',\n",
       " '本鱼栖息在在岩石附近的海草床。',\n",
       " '宋宁宗庆元元年改为嘉兴府。',\n",
       " '外交部是大韩民国政府负责外交及其他对外事务的最高机关。',\n",
       " '娶汉武帝和卫子夫之女卫长公主为妻。',\n",
       " '甘青鼠李为鼠李科鼠李属下的一个种。',\n",
       " '荷兰在所有西欧被纳粹德国占领过的国家中死亡率是最高的。',\n",
       " '喻时人。',\n",
       " '他为梵蒂冈作过画。',\n",
       " '热针的圣雷米。',\n",
       " '塞缪尔于逃走前把科尔沃放到一艘小船让其漂流离开。',\n",
       " '亚美尼亚科学院所属的布拉堪天文台位在该山的山坡上。',\n",
       " '毕业于山西大学中文专业。',\n",
       " '哈萨克斯坦地震观测和研究中心还积极寻求解决各类区域性问题。',\n",
       " '孔子大厦是一家位于美国纽约市曼哈顿唐人街的华人公寓楼。',\n",
       " '否',\n",
       " '六',\n",
       " '当地士绅张世英等也加入调停。',\n",
       " '晚年寓居尧山。',\n",
       " '锦花宝螺为宝螺科宝螺属下的一个种。',\n",
       " '劳亚龙形类的欧洲与北美洲。',\n",
       " '六',\n",
       " '否',\n",
       " '纳迪尔沙是一个杰出的统帅和无情的征服者。',\n",
       " '这使得她增加更多的歌迷。',\n",
       " '美洲拟鲽是鲽科下的一种比目鱼。',\n",
       " '岛根县立美术馆是位于日本岛根县松江市的一座美术馆。',\n",
       " '罗登贝瑞的许多概念在当时是相当先进的。',\n",
       " '环七通与东急目黑线立体交差附近为洗足站。',\n",
       " '八',\n",
       " '是',\n",
       " '四',\n",
       " '纪念碑总高度。',\n",
       " '光绪二十三年七月十日生。',\n",
       " '蒙特奈。',\n",
       " '第五任克勤郡王。',\n",
       " '他是现任新世界发展有限公司的执行副主席。',\n",
       " '它研究人们应该遵守甚么样式的道德行为准则。',\n",
       " '面是类比于胞之多面体和平面镶嵌内的二维元素。',\n",
       " '和气町是位于冈山县东南部的一个城镇。',\n",
       " '爵士乐标准常用于指代非常流行的爵士音乐作品。',\n",
       " '九',\n",
       " '一',\n",
       " '三',\n",
       " '有机硼化学或有机硼烷化学是研究这类化合物的化学。',\n",
       " '其名称来源于帝国时期的一种衔位。',\n",
       " '父亲是牧师。',\n",
       " '埃列什基伽勒是地下王国中唯一可制订法律并作出裁决之神。',\n",
       " '圣约翰斯校区是纽芬兰纪念大学的主校区。',\n",
       " '加入民盟。',\n",
       " '从过去和现在带有喜悦或是痛苦的经历中产生了欲望。',\n",
       " '该电影并成为迪丽热巴在年内连续第三部票房破亿的影片。',\n",
       " '阿拉卡蒂是巴西塞阿拉州的一个市镇。',\n",
       " '担任全国纺织工业劳模。',\n",
       " '曾祖父钱中得。',\n",
       " '饰演海砂的户田惠梨香认为月和海砂的行为皆属犯罪。',\n",
       " '是一部原住民温馨喜剧。',\n",
       " '现为天津市第一医院住院部。',\n",
       " '圆环周边是高雄市在战后的商业中心之一。',\n",
       " '基于北越南的统计数据',\n",
       " '打戏特效一百分',\n",
       " '目前栽培方式以太空包栽培法居多。',\n",
       " '零',\n",
       " '九',\n",
       " '七',\n",
       " '克劳福德福克是一个位于美国阿拉巴马州格林县的非建制地区。',\n",
       " '山梨县出身。',\n",
       " '皇室拓跋氏亦改为汉姓元姓。',\n",
       " '愿诸君勿复言。',\n",
       " '欧阳予倩的祖父。',\n",
       " '随后咸丰帝及宫廷人等前往承德避暑山庄避难。',\n",
       " '富津市是千叶县南部的一市。',\n",
       " '她的父母在爱荷华大学读本科的时候遇见了彼此并恋爱。',\n",
       " '普特南县是美国乔治亚州中部的一个县。',\n",
       " '家庭自动化。',\n",
       " '利比亚将成立一个特别委员会以执行这项政治隔离法令。',\n",
       " '白尾松田鼠为仓鼠科松田鼠属的动物。',\n",
       " '由香港赛马会慈善信托基金独家赞助。',\n",
       " '帕普利维尔在英格兰东约克郡长大。',\n",
       " '老虎头山岗新石器遗址的历史年代为新石器时代。',\n",
       " '之后有一群由琉球大学的木村政昭指挥的研究人员也证实了这些遗迹的存在。',\n",
       " '业务上受中华人民共和国财政部指导。',\n",
       " '无论如何它被置放于主题之后。',\n",
       " '康熙三十四年授千总。',\n",
       " '许多分子系统发生学的研究皆显示本属是一个有效的分类元。',\n",
       " '陈玉珍任校长。',\n",
       " '身体是透明的白色。',\n",
       " '广播音效奖为现行金钟奖颁发奖项。',\n",
       " '细带闪蛱蝶在欧洲被很严格地保护着。',\n",
       " '普斯陶森特拉斯洛。',\n",
       " '但有部份如娄底话则没有。',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = cv_zh_df['sentence'].tolist()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/siqiouyang/work/projects/WMSeg/zh.txt', 'w') as w:\n",
    "#     w.write('\\n'.join(sentences).replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = re.compile(\",|\\?|・|\\u3002|\\uff1f|\\uff01|\\uff0c|\\u3001|\\uff1b|\\uff1a|\\u201c|\\u201d|\\u2018|\\u2019|\\uff08|\\uff09|\\u300a|\\u300b|\\u3008|\\u3009|\\u3010|\\u3011|\\u300e|\\u300f|\\u300c|\\u300d|\\ufe43|\\ufe44|\\u3014|\\u3015|\\u2026|\\u2014|\\uff5e|\\ufe4f|\\uffe5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = []\n",
    "with open('/home/siqiouyang/work/projects/WMSeg/cv9_zh.txt.tok', 'r') as r:\n",
    "    for line in r.readlines():\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            line = punc.sub(' ', line)\n",
    "            tokens = [tok for tok in line.split(' ') if tok != '']\n",
    "            tokenized_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path, tokens in zip(cv_zh_df['path'], tokenized_sentences):\n",
    "#     with open(os.path.join(cv_zh_root, '16kHz', '{}.txt'.format(path[:-4])), 'w') as w:\n",
    "#         w.write(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48516/48516 [00:28<00:00, 1722.70it/s]\n"
     ]
    }
   ],
   "source": [
    "final_tokenized_sentences = []\n",
    "final_segmentations = []\n",
    "final_audio_paths = []\n",
    "for i, path in enumerate(tqdm(cv_zh_df['path'])):\n",
    "    id = path[:-4]\n",
    "    grid_path = os.path.join(cv_zh_root, '16kHz/align_wmseg/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        filtered_grid = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        if len(filtered_grid) != len(tokenized_sentences[i]):\n",
    "            print(filtered_grid, tokenized_sentences[i], sep='\\n', end='\\n' + '-'*10 + '\\n')\n",
    "\n",
    "        interval = np.array([(word.xmin, word.xmax) for word in filtered_grid])\n",
    "        audio_path = os.path.join(cv_zh_root, '16kHz/{}.wav'.format(id))\n",
    "        info = torchaudio.info(audio_path)\n",
    "        duration = info.num_frames / info.sample_rate\n",
    "        interval = interval / duration\n",
    "        final_segmentations.append(interval)\n",
    "\n",
    "        final_audio_paths.append(audio_path)\n",
    "        final_tokenized_sentences.append(tokenized_sentences[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramDictionary:\n",
    "    def __init__(self, tokenized_sentences, segmentations, audio_paths, gram=3, sep=''):\n",
    "        self.tokenized_sentences = tokenized_sentences\n",
    "\n",
    "        self.id2tok = []\n",
    "        self.tok2id = {}\n",
    "        self.embs = []\n",
    "        self.waveforms = {}\n",
    "\n",
    "        assert len(tokenized_sentences) == len(segmentations) and len(segmentations) == len(audio_paths)\n",
    "\n",
    "        for tok_sent, seg, path in zip(tqdm(tokenized_sentences), segmentations, audio_paths):\n",
    "            waveform = torchaudio.load(path)[0][0]\n",
    "            len_wf = waveform.size(0)\n",
    "            assert len(tok_sent) == len(seg)\n",
    "            for l in range(1, gram + 1):\n",
    "                for idx in range(len(tok_sent) - l + 1):\n",
    "                    tok = sep.join(tok_sent[idx : idx + l])\n",
    "                    xmin, xmax = seg[idx][0], seg[idx + l - 1][1]\n",
    "                    \n",
    "                    tok_wf = waveform[int(xmin * len_wf) : int(xmax * len_wf)]\n",
    "                    if tok not in self.tok2id:\n",
    "                        self.tok2id[tok] = len(self.id2tok)\n",
    "                        self.id2tok.append(tok)\n",
    "                        self.waveforms[self.tok2id[tok]] = [tok_wf]\n",
    "                    else:\n",
    "                        self.waveforms[self.tok2id[tok]].append(tok_wf)\n",
    "\n",
    "        batch_size = 10000\n",
    "        for idx in tqdm(range(0, len(self.id2tok), batch_size)):\n",
    "            self.embs.append(model.encode(self.id2tok[idx : idx + batch_size]))\n",
    "        \n",
    "        self.embs = np.concatenate(self.embs, axis=0)\n",
    "        self.index = faiss.IndexFlatIP(self.embs.shape[-1])\n",
    "        self.index.add(self.embs)\n",
    "\n",
    "        self.id2tok = np.array(self.id2tok)\n",
    "\n",
    "    def search(self, x, k):\n",
    "        return self.index.search(x, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_zh_dict = NgramDictionary(final_tokenized_sentences, final_segmentations, final_audio_paths, gram=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_zh_dict = th.load('dict/zh-CN_dict.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Chinese Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/data/siqiouyang/datasets/covost2/zh-CN'\n",
    "df = load_df_from_tsv(os.path.join(root, 'train_st_zh-CN_en.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = []\n",
    "for id in df['id']:\n",
    "    audio_paths.append(os.path.join(root, '16kHz/{}.wav'.format(id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['src_text'].tolist()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/siqiouyang/work/projects/WMSeg/zh.txt', 'w') as w:\n",
    "#     w.write('\\n'.join(sentences).replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = []\n",
    "with open('/home/siqiouyang/work/projects/WMSeg/zh.txt.tok', 'r') as r:\n",
    "    for line in r.readlines():\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            line = punc.sub(' ', line)\n",
    "            tokens = [tok for tok in line.split(' ') if tok != '']\n",
    "            tokenized_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id, tokens in zip(df['id'], tokenized_sentences):\n",
    "#     with open(os.path.join(root, '16kHz', '{}.txt'.format(id)), 'w') as w:\n",
    "#         w.write(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfa to force align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_grids = []\n",
    "for i, id in enumerate(df['id']):\n",
    "    grid = textgrids.TextGrid(os.path.join(root, '16kHz/align_wmseg/{}.TextGrid'.format(id)))\n",
    "    filtered_grid = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "    if len(filtered_grid) != len(tokenized_sentences[i]):\n",
    "        print(filtered_grid, tokenized_sentences[i], sep='\\n', end='\\n' + '-'*10 + '\\n')\n",
    "\n",
    "    filtered_grids.append(filtered_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentations = []\n",
    "for grid, path in zip(filtered_grids, audio_paths):\n",
    "    interval = np.array([(word.xmin, word.xmax) for word in grid])\n",
    "    info = torchaudio.info(path)\n",
    "    duration = info.num_frames / info.sample_rate\n",
    "    interval = interval / duration\n",
    "    segmentations.append(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    def __init__(self, tokenized_sentences, segmentations, audio_paths):\n",
    "        self.tokenized_sentences = tokenized_sentences\n",
    "\n",
    "        self.id2tok = []\n",
    "        self.tok2id = {}\n",
    "        self.embs = []\n",
    "        self.waveforms = {}\n",
    "\n",
    "        assert len(tokenized_sentences) == len(segmentations) and len(segmentations) == len(audio_paths)\n",
    "\n",
    "        for tok_sent, seg, path in zip(tqdm(tokenized_sentences), segmentations, audio_paths):\n",
    "            waveform = torchaudio.load(path)[0][0]\n",
    "            len_wf = waveform.size(0)\n",
    "            assert len(tok_sent) == len(seg), print(tok_sent, seg)\n",
    "            for tok, (xmin, xmax) in zip(tok_sent, seg):\n",
    "                tok_wf = waveform[int(xmin * len_wf) : int(xmax * len_wf)]\n",
    "                if tok not in self.tok2id:\n",
    "                    self.tok2id[tok] = len(self.id2tok)\n",
    "                    self.id2tok.append(tok)\n",
    "                    self.waveforms[self.tok2id[tok]] = [tok_wf]\n",
    "                else:\n",
    "                    self.waveforms[self.tok2id[tok]].append(tok_wf)\n",
    "\n",
    "        batch_size = 10000\n",
    "        for idx in tqdm(range(0, len(self.id2tok), batch_size)):\n",
    "            self.embs.append(model.encode(self.id2tok[idx : idx + batch_size]))\n",
    "        \n",
    "        self.embs = np.concatenate(self.embs, axis=0)\n",
    "        self.index = faiss.IndexFlatIP(self.embs.shape[-1])\n",
    "        self.index.add(self.embs)\n",
    "\n",
    "        self.id2tok = np.array(self.id2tok)\n",
    "\n",
    "    def search(self, x, k):\n",
    "        return self.index.search(x, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zh_dict = Dictionary(tokenized_sentences, segmentations, audio_paths)\n",
    "zh_dict = NgramDictionary(tokenized_sentences, segmentations, audio_paths, gram=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build German Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/data/siqiouyang/datasets/covost2/de'\n",
    "df = load_df_from_tsv(os.path.join(root, 'train_st_de_en.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['src_text'].tolist()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_punctuation = punctuation + '„“”‚’«»ʿ‹›‘'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = []\n",
    "for sent in sentences:\n",
    "    sent = ''.join(c for c in sent if c not in punctuation)\n",
    "    tokens = [tok for tok in sent.split(' ') if tok != '']\n",
    "    tokenized_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfa to force align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_grids = []\n",
    "audio_paths = []\n",
    "filtered_tokenized_sentences = []\n",
    "for i, id in enumerate(tqdm(df['id'])):\n",
    "    grid_path = os.path.join(root, '16kHz/align/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        filtered_grid = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        if len(filtered_grid) != len(tokenized_sentences[i]):\n",
    "            # print(filtered_grid, tokenized_sentences[i], sep='\\n', end='\\n' + '-'*10 + '\\n')\n",
    "            continue\n",
    "        \n",
    "        filtered_grids.append(filtered_grid)\n",
    "        filtered_tokenized_sentences.append(tokenized_sentences[i])\n",
    "        audio_paths.append(os.path.join(root, '16kHz/{}.wav'.format(id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentations = []\n",
    "for grid, path in zip(filtered_grids, audio_paths):\n",
    "    interval = np.array([(word.xmin, word.xmax) for word in grid])\n",
    "    info = torchaudio.info(path)\n",
    "    duration = info.num_frames / info.sample_rate\n",
    "    interval = interval / duration\n",
    "    segmentations.append(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de_dict = Dictionary(filtered_tokenized_sentences, segmentations, audio_paths)\n",
    "de_dict = NgramDictionary(filtered_tokenized_sentences, segmentations, audio_paths, gram=3, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build German Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_root = '/mnt/data/siqiouyang/datasets/covost2/de'\n",
    "de_df = load_df_from_tsv(os.path.join(de_root, 'train_st_de_en.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_sentences = de_df['src_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_tokenized_sentences = []\n",
    "for sent in de_sentences:\n",
    "    sent = ''.join(c for c in sent if c not in punctuation)\n",
    "    tokens = [tok for tok in sent.split(' ') if tok != '']\n",
    "    de_tokenized_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id, tokens in zip(de_df['id'], tokenized_sentences):\n",
    "#     with open(os.path.join(de_root, '16kHz', '{}.txt'.format(id)), 'w') as w:\n",
    "#         w.write(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_gram = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_de_tokens = set()\n",
    "de_freq = defaultdict(int)\n",
    "for tokens in de_tokenized_sentences:\n",
    "    for idx in range(len(tokens)):\n",
    "        for g in range(min(de_gram, len(tokens) - idx)):\n",
    "            token = ' '.join(tokens[idx: idx + g + 1])\n",
    "            all_de_tokens.add(token)\n",
    "            de_freq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1110670"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_total_token = sum(de_freq.values())\n",
    "n_total_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.save([all_de_tokens, de_freq], 'dict/de_token.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/siqiouyang/work/projects/mST/mST/analysis/language_transfer/mix_acoustic.ipynb Cell 47'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blei1/home/siqiouyang/work/projects/mST/mST/analysis/language_transfer/mix_acoustic.ipynb#ch0000045vscode-remote?line=0'>1</a>\u001b[0m de_embs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(\u001b[39mlist\u001b[39;49m(all_de_tokens), device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:164\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py?line=160'>161</a>\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py?line=162'>163</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py?line=163'>164</a>\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py?line=165'>166</a>\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py?line=166'>167</a>\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py?line=62'>63</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py?line=63'>64</a>\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py?line=65'>66</a>\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py?line=66'>67</a>\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py?line=68'>69</a>\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=986'>987</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=988'>989</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=989'>990</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=990'>991</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=993'>994</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=994'>995</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=995'>996</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=996'>997</a>\u001b[0m     embedding_output,\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=997'>998</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=998'>999</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=999'>1000</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1000'>1001</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1001'>1002</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1002'>1003</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1003'>1004</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1004'>1005</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1005'>1006</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1006'>1007</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1007'>1008</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1008'>1009</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=575'>576</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=576'>577</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=577'>578</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=581'>582</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=582'>583</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=583'>584</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=584'>585</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=585'>586</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=586'>587</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=587'>588</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=588'>589</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=589'>590</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=590'>591</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=591'>592</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=592'>593</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=594'>595</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=595'>596</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:513\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=509'>510</a>\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=510'>511</a>\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=512'>513</a>\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=513'>514</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=514'>515</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=515'>516</a>\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=517'>518</a>\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/transformers/modeling_utils.py:2928\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/modeling_utils.py?line=2924'>2925</a>\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/modeling_utils.py?line=2925'>2926</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m-> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/modeling_utils.py?line=2927'>2928</a>\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:526\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=523'>524</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=524'>525</a>\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=525'>526</a>\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=526'>527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:441\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=438'>439</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=439'>440</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=440'>441</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39;49m input_tensor)\n\u001b[1;32m    <a href='file:///home/siqiouyang/anaconda3/envs/st/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=441'>442</a>\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "de_embs = model.encode(list(all_de_tokens), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_embs.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = zh_dict.search(de_embs, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (D > 0.9).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_de_tokens = list(all_de_tokens)\n",
    "n_total_found = 0\n",
    "for idx in range(len(all_de_tokens)):\n",
    "    if mask[idx]:\n",
    "        n_total_found += de_freq[all_de_tokens[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_found / n_total_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.array(all_de_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = {}\n",
    "for w_de, w_zh in zip(tokens[mask], zh_dict.id2tok[I.flatten()[mask]]):\n",
    "    match[w_de] = w_zh\n",
    "    print(w_de, w_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to place a language tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Chinese Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_root = '/mnt/data/siqiouyang/datasets/covost2/zh-CN'\n",
    "zh_df = load_df_from_tsv(os.path.join(zh_root, 'train_st_zh-CN_en.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_sentences = zh_df['src_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_tokenized_sentences = []\n",
    "with open('/home/siqiouyang/work/projects/WMSeg/zh.txt.tok', 'r') as r:\n",
    "    for line in r.readlines():\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            line = punc.sub(' ', line)\n",
    "            tokens = [tok for tok in line.split(' ') if tok != '']\n",
    "            zh_tokenized_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zh_tokens = set()\n",
    "zh_freq = defaultdict(int)\n",
    "for tokens in zh_tokenized_sentences:\n",
    "    for token in tokens:\n",
    "        all_zh_tokens.add(token)\n",
    "        zh_freq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_token = sum(zh_freq.values())\n",
    "n_total_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_embs = model.encode(list(all_zh_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_embs.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = de_dict.search(zh_embs, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (D > 0.9).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zh_tokens = list(all_zh_tokens)\n",
    "n_total_found = 0\n",
    "for idx in range(len(all_zh_tokens)):\n",
    "    if mask[idx]:\n",
    "        n_total_found += zh_freq[all_zh_tokens[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_found / n_total_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.array(all_zh_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = {}\n",
    "for w_zh, w_de in zip(tokens[mask], de_dict.id2tok[I.flatten()[mask]]):\n",
    "    match[w_zh] = w_de\n",
    "    print(w_zh, w_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix Chinese Audio into German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_punctuation = punctuation + '„“”‚’«»ʿ‹›‘'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'de-zh-3g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zh_root = '/mnt/data/siqiouyang/datasets/covost2/{}'.format(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silence = th.zeros(1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pass = 0\n",
    "n_notpass = 0\n",
    "for id, transcript in zip(tqdm(de_df['id']), de_df['src_text']):\n",
    "    grid_path = os.path.join(de_zh_root, '16kHz/align/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        tokens = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        transcript = ''.join(c for c in transcript if c not in de_punctuation)\n",
    "        ori_tokens = [tok for tok in transcript.split(' ') if tok != '']\n",
    "\n",
    "        string = ' '.join([tok.text.lower() for tok in tokens])\n",
    "        ori_string = ' '.join([tok.lower() for tok in ori_tokens])\n",
    "\n",
    "        if string == ori_string:\n",
    "            audio_path = os.path.join(de_zh_root, '16kHz/{}.wav'.format(id))\n",
    "            waveform = torchaudio.load(audio_path)[0][0]\n",
    "            n_frame = waveform.size(0)\n",
    "            frame_rate = 16000\n",
    "            duration = n_frame / frame_rate\n",
    "\n",
    "            new_waveform = []\n",
    "            new_tokens = []\n",
    "            \n",
    "            intervals = (np.array([(tok.xmin, tok.xmax) for tok in tokens]) * frame_rate).astype(int)\n",
    "\n",
    "            average_volume = 0.\n",
    "            word_duration = 0.\n",
    "            for xmin, xmax in intervals:\n",
    "                word_duration += xmax - xmin\n",
    "                average_volume += waveform[xmin : xmax].abs().sum()\n",
    "            average_volume /= word_duration\n",
    "\n",
    "            mask = []\n",
    "            orig_intervals = []\n",
    "            mixed_intervals = []\n",
    "            last_end = idx = 0\n",
    "            prefix_length = 0.\n",
    "            while idx < len(intervals):\n",
    "                xmin, xmax = intervals[idx]\n",
    "                new_waveform.append(waveform[last_end : xmin])\n",
    "                prefix_length += xmin - last_end\n",
    "                replace = False\n",
    "                for g in range(min(de_gram, len(intervals) - idx), 0, -1):\n",
    "                    token = ' '.join(ori_tokens[idx : idx + g])\n",
    "                    if token in match:\n",
    "                        replace = True\n",
    "                        token_zh = match[token]\n",
    "                        selectable_waveforms = zh_dict.waveforms[zh_dict.tok2id[token_zh]]\n",
    "\n",
    "                        selected_waveform = random.choice(selectable_waveforms)\n",
    "                        selected_volume = selected_waveform.abs().mean()\n",
    "                        selected_waveform *= average_volume / selected_volume\n",
    "\n",
    "                        sil_selected_waveform = th.cat([silence, selected_waveform, silence], dim=0)\n",
    "\n",
    "                        new_waveform.append(sil_selected_waveform)\n",
    "                        new_tokens.append(token_zh)\n",
    "                        last_end = intervals[idx + g - 1][1]\n",
    "                        idx += g\n",
    "                        mask.append(True)\n",
    "                        orig_intervals.append((xmin, last_end))\n",
    "                        mixed_intervals.append((\n",
    "                            prefix_length + silence.size(0), \n",
    "                            prefix_length + silence.size(0) + selected_waveform.size(0)\n",
    "                        ))\n",
    "                        prefix_length += silence.size(0) * 2 + selected_waveform.size(0)\n",
    "                        break\n",
    "                if not replace:\n",
    "                    new_waveform.append(waveform[xmin : xmax])\n",
    "                    new_tokens.append(ori_tokens[idx])\n",
    "                    last_end = xmax\n",
    "                    idx += 1\n",
    "                    mask.append(False)\n",
    "                    orig_intervals.append((xmin, xmax))\n",
    "                    mixed_intervals.append((\n",
    "                        prefix_length,\n",
    "                        prefix_length + xmax - xmin\n",
    "                    ))\n",
    "                    prefix_length += xmax - xmin\n",
    "\n",
    "            new_waveform = th.cat(new_waveform, dim=0).unsqueeze(0)\n",
    "            new_audio_path = os.path.join(de_zh_root, '16kHz/{}-mixed.wav'.format(id))\n",
    "            torchaudio.save(new_audio_path, new_waveform, sample_rate=16000)\n",
    "\n",
    "            mask = th.tensor(mask, dtype=bool)\n",
    "            orig_intervals = th.tensor(orig_intervals, dtype=float) / n_frame\n",
    "            mixed_intervals = th.tensor(mixed_intervals, dtype=float) / new_waveform.size(1)\n",
    "            match_path = os.path.join(de_zh_root, '16kHz/{}.pt'.format(id))\n",
    "            th.save([mask, orig_intervals, mixed_intervals], match_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_matched = 0\n",
    "for id, transcript in zip(tqdm(de_df['id']), de_df['src_text']):\n",
    "    grid_path = os.path.join(de_zh_root, '16kHz/align/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        tokens = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        transcript = ''.join(c for c in transcript if c not in de_punctuation)\n",
    "        ori_tokens = [tok for tok in transcript.split(' ') if tok != '']\n",
    "\n",
    "        string = ' '.join([tok.text.lower() for tok in tokens])\n",
    "        ori_string = ' '.join([tok.lower() for tok in ori_tokens])\n",
    "\n",
    "        if string == ori_string:\n",
    "            for token in ori_tokens:\n",
    "                replace_flag = np.random.rand() > 0.5\n",
    "                if token in match and replace_flag:\n",
    "                    n_total_matched += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_matched / n_total_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zh_df1 = load_df_from_tsv(os.path.join(de_zh_root, 'train_st_de-zh-3g_en.tsv'))\n",
    "de_zh_df2 = load_df_from_tsv(os.path.join(de_zh_root, 'train_st_de-zh-3g_en_phone.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain = []\n",
    "for id in de_zh_df1['id']:\n",
    "    if os.path.exists(os.path.join(de_zh_root, '16kHz/{}-mixed.wav'.format(id))):\n",
    "        retain.append(True)\n",
    "    else:\n",
    "        retain.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zh_df1_filtered = de_zh_df1.loc[retain]\n",
    "de_zh_df2_filtered = de_zh_df2.loc[retain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_zh_df1_filtered['src_lang'] = [version] * len(de_zh_df1_filtered)\n",
    "de_zh_df2_filtered['src_lang'] = [version] * len(de_zh_df2_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_to_tsv(de_zh_df1_filtered, os.path.join(de_zh_root, 'train_st_{}_en.tsv'.format(version)))\n",
    "save_df_to_tsv(de_zh_df2_filtered, os.path.join(de_zh_root, 'train_st_{}_en_phone.tsv'.format(version)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix German Audio into Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_de_root = '/mnt/data/siqiouyang/datasets/covost2/zh-de-3g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pass = 0\n",
    "n_notpass = 0\n",
    "for id, ori_tokens in zip(tqdm(zh_df['id']), zh_tokenized_sentences):\n",
    "    grid_path = os.path.join(zh_de_root, '16kHz/align_wmseg/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        tokens = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        assert len(tokens) == len(ori_tokens)\n",
    "\n",
    "        audio_path = os.path.join(zh_de_root, '16kHz/{}.wav'.format(id))\n",
    "        waveform = torchaudio.load(audio_path)[0][0]\n",
    "        n_frame = waveform.size()\n",
    "        frame_rate = 16000\n",
    "\n",
    "        new_waveform = []\n",
    "        \n",
    "        intervals = (np.array([(tok.xmin, tok.xmax) for tok in tokens]) * frame_rate).astype(int)\n",
    "        last_end = 0\n",
    "        for token, (xmin, xmax) in zip(ori_tokens, intervals):\n",
    "            new_waveform.append(waveform[last_end : xmin])\n",
    "\n",
    "            if token in match:\n",
    "                token_de = match[token]\n",
    "                selectable_waveforms = de_dict.waveforms[de_dict.tok2id[token_de]]\n",
    "                new_waveform.append(random.choice(selectable_waveforms))\n",
    "            else:\n",
    "                new_waveform.append(waveform[xmin : xmax])\n",
    "\n",
    "            last_end = xmax\n",
    "\n",
    "        new_waveform = th.cat(new_waveform, dim=0).unsqueeze(0)\n",
    "        torchaudio.save(audio_path, new_waveform, sample_rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust Number of Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'de-zh-3g-hf'\n",
    "adj_root = '/mnt/data/siqiouyang/datasets/covost2/' + version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_df1 = load_df_from_tsv(os.path.join(adj_root, 'train_st_{}_en.tsv'.format(version)))\n",
    "adj_df2 = load_df_from_tsv(os.path.join(adj_root, 'train_st_{}_en_phone.tsv'.format(version)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adj_df in [adj_df1, adj_df2]:\n",
    "    n_frames = []\n",
    "    for id in tqdm(adj_df['id']):\n",
    "        info = torchaudio.info(os.path.join(adj_root, '16kHz/{}.wav'.format(id)))\n",
    "        n_frames.append(info.num_frames)\n",
    "    adj_df['n_frames'] = n_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_to_tsv(adj_df1, os.path.join(adj_root, 'train_st_{}_en.tsv'.format(version)))\n",
    "save_df_to_tsv(adj_df2, os.path.join(adj_root, 'train_st_{}_en_phone.tsv'.format(version)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_punctuation = punctuation + '„“”‚’«»ʿ‹›‘'\n",
    "de_zh_root = '/mnt/data/siqiouyang/datasets/covost2/de'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(de_df)))\n",
    "random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silence = th.zeros(1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_replaced = 0\n",
    "n_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_origs = []\n",
    "de_mixeds = []\n",
    "de_audios = []\n",
    "de_orig_audios = []\n",
    "for idx in tqdm(indices[:n_max]):\n",
    "    id = de_df['id'][idx]\n",
    "    transcript = de_df['src_text'][idx]\n",
    "    grid_path = os.path.join(de_zh_root, '16kHz/align/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        tokens = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        transcript = ''.join(c for c in transcript if c not in de_punctuation)\n",
    "        ori_tokens = [tok for tok in transcript.split(' ') if tok != '']\n",
    "\n",
    "        string = ' '.join([tok.text.lower() for tok in tokens])\n",
    "        ori_string = ' '.join([tok.lower() for tok in ori_tokens])\n",
    "\n",
    "        if string == ori_string:\n",
    "            audio_path = os.path.join(de_zh_root, '16kHz/{}.wav'.format(id))\n",
    "            waveform = torchaudio.load(audio_path)[0][0]\n",
    "            n_frame = waveform.size()\n",
    "            frame_rate = 16000\n",
    "\n",
    "            new_waveform = []\n",
    "            new_tokens = []\n",
    "            \n",
    "            intervals = (np.array([(tok.xmin, tok.xmax) for tok in tokens]) * frame_rate).astype(int)\n",
    "\n",
    "            average_volume = 0.\n",
    "            total_duration = 0.\n",
    "            for xmin, xmax in intervals:\n",
    "                total_duration += xmax - xmin\n",
    "                average_volume += waveform[xmin : xmax].abs().sum()\n",
    "            average_volume /= total_duration\n",
    "\n",
    "            last_end = 0\n",
    "\n",
    "            idx = 0\n",
    "            while idx < len(intervals):\n",
    "                xmin, xmax = intervals[idx]\n",
    "                new_waveform.append(waveform[last_end : xmin])\n",
    "                replace = False\n",
    "                for g in range(min(de_gram, len(intervals) - idx), 0, -1):\n",
    "                    token = ' '.join(ori_tokens[idx : idx + g])\n",
    "                    if token in match:\n",
    "                        replace = True\n",
    "                        token_zh = match[token]\n",
    "                        selectable_waveforms = zh_dict.waveforms[zh_dict.tok2id[token_zh]]\n",
    "\n",
    "                        selected_waveform = random.choice(selectable_waveforms)\n",
    "                        selected_volume = selected_waveform.abs().mean()\n",
    "                        selected_waveform *= average_volume / selected_volume\n",
    "\n",
    "                        selected_waveform = th.cat([silence, selected_waveform, silence], dim=0)\n",
    "\n",
    "                        new_waveform.append(selected_waveform)\n",
    "                        new_tokens.append(token_zh)\n",
    "                        last_end = intervals[idx + g - 1][1]\n",
    "                        idx += g\n",
    "                        n_replaced += g\n",
    "                        n_total += g\n",
    "                        break\n",
    "                if not replace:\n",
    "                    new_waveform.append(waveform[xmin : xmax])\n",
    "                    new_tokens.append(ori_tokens[idx])\n",
    "                    last_end = xmax\n",
    "                    idx += 1\n",
    "                    n_total += 1\n",
    "\n",
    "            # for token, (xmin, xmax) in zip(ori_tokens, intervals):\n",
    "            #     new_waveform.append(waveform[last_end : xmin])\n",
    "\n",
    "            #     replace_flag = True # np.random.rand() > 0.5\n",
    "\n",
    "            #     if token in match and replace_flag:\n",
    "            #         token_zh = match[token]\n",
    "            #         selectable_waveforms = zh_dict.waveforms[zh_dict.tok2id[token_zh]]\n",
    "            #         new_waveform.append(random.choice(selectable_waveforms))\n",
    "            #         new_tokens.append(match[token])\n",
    "            #     else:\n",
    "            #         new_waveform.append(waveform[xmin : xmax])\n",
    "            #         new_tokens.append(token)\n",
    "\n",
    "            #     last_end = xmax\n",
    "\n",
    "            new_waveform = th.cat(new_waveform, dim=0).unsqueeze(0)\n",
    "            new_audio_path = 'de-zh-sample/{}.wav'.format(len(de_origs))\n",
    "            orig_audio_path = 'de-zh-sample/{}-orig.wav'.format(len(de_origs))\n",
    "            torchaudio.save(new_audio_path, new_waveform, sample_rate=16000)\n",
    "            torchaudio.save(orig_audio_path, waveform.unsqueeze(0), sample_rate=16000)\n",
    "\n",
    "            de_origs.append(' '.join(ori_tokens))\n",
    "            de_mixeds.append(' '.join(new_tokens))\n",
    "            de_audios.append(new_audio_path)\n",
    "            de_orig_audios.append(orig_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_replaced / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '<table>\\n'\n",
    "string += '\\t<tr>\\n\\t\\t<th>de-orig</th>\\n\\t\\t<th>audio-orig</th>\\n\\t\\t<th>de-mixed</th>\\n\\t\\t<th>audio-mixed</th>\\n\\t</tr>\\n'\n",
    "for de_orig, de_mixed, de_audio, de_orig_audio in zip(de_origs, de_mixeds, de_audios, de_orig_audios):\n",
    "    string += '\\t<tr>\\n\\t\\t<td>{}</td>\\n\\t\\t<td>{}</td>\\n\\t\\t<td>{}</td>\\n\\t\\t<td>{}</td>\\n\\t</tr>\\n'.format(\n",
    "        de_orig, \n",
    "        '<audio controls><source src=\"{}\" type=\"audio/wav\"></audio>'.format(de_orig_audio),\n",
    "        de_mixed,\n",
    "        '<audio controls><source src=\"{}\" type=\"audio/wav\"></audio>'.format(de_audio)\n",
    "    )\n",
    "string += '</table>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('de-zh-sample.html', 'w', encoding='utf-8') as w:\n",
    "    w.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_origs = []\n",
    "de_orig_intervals = []\n",
    "de_mixeds = []\n",
    "de_mixed_intervals = []\n",
    "de_audios = []\n",
    "de_orig_audios = []\n",
    "for idx in tqdm(indices[:n_max]):\n",
    "    id = de_df['id'][idx]\n",
    "    transcript = de_df['src_text'][idx]\n",
    "    grid_path = os.path.join(de_zh_root, '16kHz/align/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        tokens = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        transcript = ''.join(c for c in transcript if c not in de_punctuation)\n",
    "        ori_tokens = [tok for tok in transcript.split(' ') if tok != '']\n",
    "\n",
    "        string = ' '.join([tok.text.lower() for tok in tokens])\n",
    "        ori_string = ' '.join([tok.lower() for tok in ori_tokens])\n",
    "\n",
    "        if string == ori_string:\n",
    "            audio_path = os.path.join(de_zh_root, '16kHz/{}.wav'.format(id))\n",
    "            waveform = torchaudio.load(audio_path)[0][0]\n",
    "            n_frame = waveform.size()\n",
    "            frame_rate = 16000\n",
    "\n",
    "            new_waveform = []\n",
    "            new_tokens = []\n",
    "\n",
    "            orig_intervals = []\n",
    "            new_intervals = []\n",
    "            \n",
    "            intervals = (np.array([(tok.xmin, tok.xmax) for tok in tokens]) * frame_rate).astype(int)\n",
    "            last_end = 0\n",
    "            sum_lengths = 0\n",
    "            for token, (xmin, xmax) in zip(ori_tokens, intervals):\n",
    "                new_waveform.append(waveform[last_end : xmin])\n",
    "                sum_lengths += xmin - last_end\n",
    "\n",
    "                replace_flag = True # np.random.rand() > 0.5\n",
    "\n",
    "                orig_intervals.append((xmin, xmax))\n",
    "\n",
    "                if token in match and replace_flag:\n",
    "                    token_zh = match[token]\n",
    "                    selectable_waveforms = zh_dict.waveforms[zh_dict.tok2id[token_zh]]\n",
    "                    selected_waveform = random.choice(selectable_waveforms)\n",
    "                    new_waveform.append(selected_waveform)\n",
    "                    new_tokens.append(match[token])\n",
    "                    new_intervals.append((sum_lengths, sum_lengths + selected_waveform.size(0)))\n",
    "                    sum_lengths += selected_waveform.size(0)\n",
    "                else:\n",
    "                    new_waveform.append(waveform[xmin : xmax])\n",
    "                    new_tokens.append(token)\n",
    "                    new_intervals.append((sum_lengths, sum_lengths + xmax - xmin))\n",
    "                    sum_lengths += xmax - xmin\n",
    "\n",
    "                last_end = xmax\n",
    "\n",
    "            new_waveform = th.cat(new_waveform, dim=0).unsqueeze(0)\n",
    "            new_audio_path = 'de-zh-sample/{}.wav'.format(len(de_origs))\n",
    "            orig_audio_path = 'de-zh-sample/{}-orig.wav'.format(len(de_origs))\n",
    "            torchaudio.save(new_audio_path, new_waveform, sample_rate=16000)\n",
    "            torchaudio.save(orig_audio_path, waveform.unsqueeze(0), sample_rate=16000)\n",
    "\n",
    "            de_origs.append(ori_tokens)\n",
    "            de_orig_intervals.append(orig_intervals)\n",
    "            de_mixeds.append(new_tokens)\n",
    "            de_mixed_intervals.append(new_intervals)\n",
    "            de_audios.append(new_audio_path)\n",
    "            de_orig_audios.append(orig_audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "task = Namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(vocab_filename):\n",
    "    _dict_path = vocab_filename\n",
    "    if not os.path.isfile(_dict_path):\n",
    "        raise FileNotFoundError(f\"Dict not found: {_dict_path}\")\n",
    "    _dict = Dictionary.load(_dict_path)\n",
    "    for code in codes:\n",
    "        _dict.add_symbol(MultilingualTripletDataset.LANG_TAG_TEMPLATE.format(code))\n",
    "    _dict.add_symbol('<mask>')\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_list_filename = '/mnt/data/siqiouyang/runs/mST/pretrained/mbart50.ft.n1/ML50_langs.txt'\n",
    "vocab_filename = '/mnt/data/siqiouyang/runs/mST/pretrained/mbart50.ft.n1/dict.txt'\n",
    "phone_vocab_filename = '/mnt/data/siqiouyang/datasets/covost2/phone_dict.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = MultilingualTripletDataset.get_lang_codes(lang_list_filename)\n",
    "dict = load_dict(vocab_filename)\n",
    "with open(phone_vocab_filename, 'r') as r:\n",
    "    phone_list = [l.strip() for l in r.readlines() if l.strip() != '']\n",
    "    phone_dict = {l: idx + 1 for idx, l in enumerate(phone_list)} # leave 0 as blank\n",
    "    phone_list = ['|'] + phone_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.src_dict = task.tgt_dict = dict\n",
    "task.phone_dict = phone_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.w2v2_model_path = '/mnt/data/siqiouyang/runs/mST/pretrained/xlsr2_300m.pt'\n",
    "args.mbart50_dir = '/mnt/data/siqiouyang/runs/mST/pretrained/mbart50.ft.n1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = W2V2Transformer.build_model(args, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/mnt/data/siqiouyang/runs/mST/xlsr_phone_mbart_zh_de-zh/checkpoint_13_10000.pt'\n",
    "ckpt = load_checkpoint_to_cpu(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict.symbols[-55:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(path, lang_tag):\n",
    "    source = get_features_or_waveform(\n",
    "        path,\n",
    "        need_waveform=True,\n",
    "        sample_rate=16000,\n",
    "    )\n",
    "    source = th.from_numpy(source).float()\n",
    "    src_tokens = source.unsqueeze(0).to(device)\n",
    "    src_lengths = th.LongTensor([source.size(0)]).to(device)\n",
    "    src_lang_tag_idx = dict.index(lang_tag)\n",
    "    src_lang_tag_indices = th.LongTensor([src_lang_tag_idx]).unsqueeze(-1).to(device)\n",
    "    encoder_out = model.encoder(src_tokens, src_lengths, src_lang_tag_indices)\n",
    "    return encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'de-zh-sample/1-orig.wav'\n",
    "info = torchaudio.info(path)\n",
    "orig_encoder_out = compute(path, '<lang:de_DE>')\n",
    "orig_length, _, d = orig_encoder_out.encoder_out.size()\n",
    "orig_interval = ((np.array(de_orig_intervals[1]) / info.num_frames) * orig_length).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'de-zh-sample/1.wav'\n",
    "info = torchaudio.info(path)\n",
    "mixed_encoder_out = compute(path, '<lang:de_DE>')\n",
    "mixed_length, _, d = mixed_encoder_out.encoder_out.size()\n",
    "mixed_interval = ((np.array(de_orig_intervals[1]) / info.num_frames) * mixed_length).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = orig_encoder_out.encoder_out[47:56].mean(dim=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = mixed_encoder_out.encoder_out[56:65].mean(dim=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = mixed_encoder_out.encoder_out[47:55].mean(dim=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A * B).sum() / A.norm() / B.norm(), ((A - B) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A * C).sum() / A.norm() / C.norm(), ((A - C) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27e528650793650579273e035796a30e6ea4eb9cce70b5c065b2912b46e06367"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('st')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
